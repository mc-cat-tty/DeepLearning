\section{PyTorch Logistic Regression}
View source \href{https://github.com/mc-cat-tty/DeepLearning/blob/main/labs/2-logreg.ipynb}{here}.

\subsection{Torch Optimizers}
Blast everything seen so far. We can optimize parameters with the \texttt{torch.optim} package.
We can choose between different optimizers: most common are SGD and ADAM. Example:
\begin{verbatim}
optimizer = torch.optim.SGD([w], learning_rate)
[...]
loss.backward()
optimizer.step()
optimizer.zero_grad()
\end{verbatim}