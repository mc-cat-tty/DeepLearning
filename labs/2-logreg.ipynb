{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16716,
     "status": "ok",
     "timestamp": 1759418137569,
     "user": {
      "displayName": "Angelo Porrello",
      "userId": "16647851955948411858"
     },
     "user_tz": -120
    },
    "id": "NFDsAyCeBg1E",
    "outputId": "b0129d23-d6e8-4b0b-f517-68d06a54799e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1053bbef0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import gdown\n",
    "#from googledrivedownloader import GoogleDriveDownloader\n",
    "#import GoogleDriveDownloader\n",
    "import zipfile\n",
    "\n",
    "eps_torch = torch.finfo(float).eps\n",
    "\n",
    "torch.manual_seed(191090)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5-zINHEkkug"
   },
   "source": [
    "# Core goals of the lab\n",
    "1) Learn how to use torch.optim instead of manual parameter updates\n",
    "2) Implement logistic regression (useful for classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1728,
     "status": "ok",
     "timestamp": 1759419114212,
     "user": {
      "displayName": "Angelo Porrello",
      "userId": "16647851955948411858"
     },
     "user_tz": -120
    },
    "id": "GSJ8YQ7SH7OQ",
    "outputId": "4e412956-5e08-4653-9513-1d6b2017681c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1SagLh5XNSV4znhlnkLRkV7zHPSDbOAqv\n",
      "To: /Users/francesco/Desktop/Courses/Notes/DeepLearning/labs/got.zip\n",
      "100%|██████████| 84.6k/84.6k [00:00<00:00, 7.26MB/s]\n"
     ]
    }
   ],
   "source": [
    "gdown.download(f\"https://drive.google.com/uc?id=1SagLh5XNSV4znhlnkLRkV7zHPSDbOAqv\",\n",
    "               output=\"./got.zip\", quiet=False)\n",
    "\n",
    "with zipfile.ZipFile(\"got.zip\", 'r') as zip_ref:\n",
    "  zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1759418353604,
     "user": {
      "displayName": "Angelo Porrello",
      "userId": "16647851955948411858"
     },
     "user_tz": -120
    },
    "id": "ZVEt31u-BrDN"
   },
   "outputs": [],
   "source": [
    "def load_got_dataset(path, train_split=0.8, verbose=True):\n",
    "  \"\"\"\n",
    "  Loads the Game of Thrones dataset.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  path: str\n",
    "      the relative path of the csv file.\n",
    "  train_split: float\n",
    "      percentage of training examples in [0, 1].\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  tuple\n",
    "      x_train: np.array\n",
    "          training characters. shape=(n_train_examples, n_features)\n",
    "      y_train: np.array\n",
    "          training labels. shape=(n_train_examples,)\n",
    "      train_names: np.array\n",
    "          training names. shape=(n_train_examples,)\n",
    "      x_test: np.array\n",
    "          test characters. shape=(n_test_examples, n_features)\n",
    "      y_test: np.array\n",
    "          test labels. shape=(n_test_examples,)\n",
    "      test_names: np.array\n",
    "          test names. shape=(n_test_examples,)\n",
    "      feature_names: np.array\n",
    "          an array explaining each feature. shape=(n_test_examples,)\n",
    "  \"\"\"\n",
    "\n",
    "  # read file into string ndarray\n",
    "  with open(path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    data = np.array([row for row in reader])\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"\\nLoaded dataset from {path}\")\n",
    "    print(f\"Shape: {data.shape[0]} rows Ã— {data.shape[1]} columns\")\n",
    "\n",
    "    # print header\n",
    "    header = data[0]\n",
    "    print(\"Columns:\", \", \".join(header))\n",
    "\n",
    "    # print a preview of first 5 rows\n",
    "    print(\"\\nSample rows:\")\n",
    "    for row in data[1:6]:\n",
    "      print(\"  \", row)\n",
    "\n",
    "  # extract feature names\n",
    "  feature_names = data[0, 1:-1]\n",
    "\n",
    "  # shuffle data\n",
    "  data = data[1:]\n",
    "  np.random.shuffle(data)\n",
    "\n",
    "  # extract character names\n",
    "  character_names = data[:, 0]\n",
    "\n",
    "  # extract features X and targets Y\n",
    "  X = np.float32(data[:, 1:-1])\n",
    "  Y = np.float32(data[:, -1])\n",
    "\n",
    "  # normalize X\n",
    "  X -= np.min(X, axis=0)\n",
    "  X /= np.max(X, axis=0)\n",
    "\n",
    "  # add bias to X\n",
    "  X = np.concatenate((X, np.ones(shape=(X.shape[0], 1))), axis=1)\n",
    "  feature_names = np.concatenate((feature_names, np.array(['bias'])), axis=-1)\n",
    "\n",
    "  total_characters = X.shape[0]\n",
    "  test_sampling_probs = np.ones(shape=total_characters)\n",
    "  test_sampling_probs[Y == 1] /= float(np.sum(Y == 1))\n",
    "  test_sampling_probs[Y == 0] /= float(np.sum(Y == 0))\n",
    "  test_sampling_probs /= np.sum(test_sampling_probs)\n",
    "\n",
    "  # sample test people without replacement\n",
    "  n_test_characters = int(total_characters * (1 - train_split))\n",
    "  test_idx = np.random.choice(np.arange(0, total_characters), size=(n_test_characters,),\n",
    "                              replace=False, p=test_sampling_probs)\n",
    "  x_test = X[test_idx]\n",
    "  y_test = Y[test_idx]\n",
    "  test_names = character_names[test_idx]\n",
    "\n",
    "  # sample train people\n",
    "  train_sampling_probs = test_sampling_probs.copy()\n",
    "  train_sampling_probs[test_idx] = 0\n",
    "  train_sampling_probs /= np.sum(train_sampling_probs)\n",
    "\n",
    "  n_train_characters = int(total_characters * train_split)\n",
    "  train_idx = np.random.choice(np.arange(0, total_characters), size=(n_train_characters,),\n",
    "                                replace=True, p=train_sampling_probs)\n",
    "  x_train = X[train_idx]\n",
    "  y_train = Y[train_idx]\n",
    "  train_names = character_names[train_idx]\n",
    "\n",
    "  return x_train, y_train, train_names, x_test, y_test, test_names, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1759418398248,
     "user": {
      "displayName": "Angelo Porrello",
      "userId": "16647851955948411858"
     },
     "user_tz": -120
    },
    "id": "d_4R-fb8BrA1",
    "outputId": "3d18acd6-355c-4bff-edea-a7e36b2407db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded dataset from got.csv\n",
      "Shape: 1947 rows Ã— 27 columns\n",
      "Columns: name, male, numDeadRelations, book1, book2, book3, book4, book5, bookCount, isMarried, isPopular, witnessed_wins, witnessed_losses, hadMoreWinsThanLosses, wasAttackerCommander, wasDefenderCommander, wasCommander, witnessed_own_attacker_size_mean, witnessed_opponent_attacker_size_mean, witnessed_own_defender_size_mean, witnessed_opponent_defender_size_mean, witnessed_major_deaths, witnessed_major_capture, battleCountAsAttackerCommander, battleCountAsDefenderCommander, battleCountAsCommander, isAlive\n",
      "\n",
      "Sample rows:\n",
      "   ['Viserys II Targaryen' '1' '11' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      "   ['Walder Frey' '1' '1' '1' '1' '1' '1' '1' '5' '1' '1' '3' '0' '1' '1' '0'\n",
      " '1' '3166' '0' '0' '1166' '1' '2' '3' '0' '3' '1']\n",
      "   ['Addison Hill' '1' '0' '0' '0' '0' '1' '0' '1' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1']\n",
      "   ['Aemma Arryn' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      "   ['Sylva Santagar' '0' '0' '0' '0' '0' '1' '0' '1' '1' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1']\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, train_names, x_test, y_test, test_names, feature_names = load_got_dataset(path='got.csv', train_split=0.8)\n",
    "\n",
    "#convert from np_array to tensors\n",
    "x_train = torch.from_numpy(x_train).to(dtype=torch.float32)\n",
    "x_test = torch.from_numpy(x_test).to(dtype=torch.float32)\n",
    "y_train = torch.from_numpy(y_train).to(dtype=torch.float32)\n",
    "y_test = torch.from_numpy(y_test).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Jy4EGTv8Bq8Z"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: torch.Tensor) -> torch.Tensor:\n",
    "  e = torch.exp(x)\n",
    "  return e / (1 + e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0RWNLmKBq5w"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "  \"\"\" Models a logistic regression classifier. \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self._w = None\n",
    "    self.optim = None\n",
    "    self.loss = None\n",
    "\n",
    "  def fit_sgd(self, X, Y, n_epochs, learning_rate, verbose=False):\n",
    "      \"\"\"\n",
    "      Implements the stochastic gradient descent training procedure.\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      X: torch.tensor\n",
    "          data. shape=(n_examples, n_features)\n",
    "      Y: np.array\n",
    "          labels. shape=(n_examples,)\n",
    "      n_epochs: int\n",
    "          number of gradient updates.\n",
    "      learning_rate: float\n",
    "          step towards the descent.\n",
    "      verbose: bool\n",
    "          whether or not to print the value of cost function.\n",
    "      \"\"\"\n",
    "\n",
    "      n_samples, n_features = X.shape\n",
    "\n",
    "      # weight initialization\n",
    "      self._w = torch.randn(n_features, requires_grad=True)\n",
    "\n",
    "      # optimizer initialization\n",
    "      self.optim = torch.optim.SGD([self._w], learning_rate)\n",
    "\n",
    "      # loss initialization\n",
    "      self.loss_f = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "      for e in range(n_epochs):\n",
    "        # Empy optimizer gradient buffer\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        # Compute predictions\n",
    "        preds = sigmoid(X @ self._w)\n",
    "\n",
    "        # Print loss between Y and predictions p\n",
    "        loss = self.loss_f(preds, Y)\n",
    "\n",
    "        if verbose and e % 500 == 0:\n",
    "          print(f'Epoch {e:4d}: loss={loss}')\n",
    "\n",
    "        # Gradient backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameters update\n",
    "        self.optim.step()\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Function that predicts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: torch.tensor\n",
    "        data to be predicted. shape=(n_test_examples, n_features)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prediction: torch.tensor\n",
    "        prediction in {0, 1}.\n",
    "        Shape is (n_test_examples,)\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "      cont_preds = sigmoid(X @ self._w.T)\n",
    "      discrete_preds = torch.round(cont_preds)\n",
    "      return discrete_preds\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "9kJhGRq8Bq3R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: loss=1.2299299240112305\n",
      "Epoch  500: loss=0.8214868307113647\n",
      "Epoch 1000: loss=0.7468236088752747\n",
      "Epoch 1500: loss=0.7009949088096619\n",
      "Epoch 2000: loss=0.6715631484985352\n",
      "Epoch 2500: loss=0.6519611477851868\n",
      "Epoch 3000: loss=0.6383994221687317\n",
      "Epoch 3500: loss=0.6287004947662354\n",
      "Epoch 4000: loss=0.6215760111808777\n",
      "Epoch 4500: loss=0.6162293553352356\n",
      "Epoch 5000: loss=0.6121455430984497\n",
      "Epoch 5500: loss=0.6089779734611511\n",
      "Epoch 6000: loss=0.6064861416816711\n",
      "Epoch 6500: loss=0.6044992208480835\n",
      "Epoch 7000: loss=0.6028935313224792\n",
      "Epoch 7500: loss=0.601578414440155\n",
      "Epoch 8000: loss=0.6004866361618042\n",
      "Epoch 8500: loss=0.5995681881904602\n",
      "Epoch 9000: loss=0.5987850427627563\n",
      "Epoch 9500: loss=0.5981085896492004\n",
      "Test accuracy: 0.6915167095115681\n"
     ]
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression()\n",
    "\n",
    "# train\n",
    "logistic_reg.fit_sgd(x_train, y_train, n_epochs=10000, learning_rate=0.01, verbose=True)\n",
    "\n",
    "# test\n",
    "predictions = logistic_reg.predict(x_test)\n",
    "\n",
    "accuracy = float(torch.sum(predictions == y_test)) / y_test.shape[0]\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
