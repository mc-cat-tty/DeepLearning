\section{Transformer}
In TCN we have an inductive bias. Current values depend on past ones. This enforces causality.
Moreover, there are cases in which output could depend on non-local features, such as in coding, in which
the logic of a piece of code could depend all the previous variables.

We want a more flexible architecture.
Transformers are composed of an encoder and a decoder.

ENCODER.

The encoder consists of multiple attention blocks. Each block mantains the dimension of the input, differntly from a convolution.
Both multi head attention and feed forward can be skipped with residual connections.

Add and normalize is layer with an add, necessary for the residual, and a normalize layer, that we will se later.

Layer normalization.
We have already seen batch normalization... computing running statistics to avoid ICS.
Instead, in layer normalization we compute mean and std dev of the input tokens. Then, each token in normalized subtracting the mean
and dividing by the standard deviation.

TODO formula. Hadamard product. Gamma beta learnable.

Feed forward neural network are two linear layers with ReLU activation. The size of the layers are designed in order to move
to the original size of the input.

Multi Head Attention (MHA).
This is where the magic happens. MHA serves the purpose stated at the beginning. We want a dynamical receptive field that can shrink or grow
autonomously.

MHA doesnt change the dimension of the input vector. Given T input tokens, the same number of T tokens is returned as output.
In the output vector, each token is a weighted sum of input tokens. We want how to weight the input tokens. This is the only learnable parameter of the layer.

Attention can be seen as a Content-Based Memory Access. The model learns an association key-value: the model mantains memory slots represented
as key value pairs, it generats a query vector that describes what is looking for, then the similarity between the query and each key is computed.

It is basically an associative memory.

TODO: query keys and values

Input X is multiplied by Wq to get the query matrix. X is composed of a token for each row, with features on the columns. The output matrix
Q has a row for each token.

The key K is get by the matrix mul of X and Wk... Dimensions same as before.

This is called self-attenion since both the keys and the query are computed from the inputs.

See: cross attention

TODO: attention formula. The idea is to compute the similarity between the query and the key by multiplying each row of the query by the respective row of the key.
As a result, I obtain a similarity matrix that has on the diagional the similarity of each element with itself.
We pass it to a softmax so that the sum of the items sums to one.

Problem: the output of the attention must be summed with the residual, dimension must be adjusted. TODO how? concatenation?

MHA layers are usually more than one.

As output, we have the same number of tokens given as input. This is not good for classification:
 - take average of all the embeddings and give it as input to a MLP.
 - CLS token: a fake input token, chose as random value in original version; its output counterpart is used as the class, all the rest is thrown away.

What about training? all the operations seen so far are linear.

Loss function for regresssion.
MSE. TODO formula. The square lowers a lot small values.

Sometimes MAE is used.

Binning.
Instead of doing regression in continous way we can use binning: bin continous near values.
Binning acts like a discretization of the input. We can then solve a classification problem

TODO: why binning?

Remember: classification is always easier than regression.
Remember: both MAE and MSE equally penalize overestimating and underestimating values.

In real world problems, underestimating can be worse than overestimating. E.g., risks should be overestimated, ecc
In these cases quantile loss is used.
TODO: formula fo quantile loss

Depending on the tau we can decide to penalize more over or under estimation.
