\section{Refresher}
\subsection{Logistic Regression}
We are in the \textbf{supervised learning} case.\\
The classification is \textbf{binary}, meaning that the Y class will be just one or zero.

Formally, we are given a training set of $N$ training examples, made up of $m$ features and a label each:
$\{X_i, Y_i\}_{i=1}^N$ where $X_i \in \mathbb{R}^m$ and $Y_i \in \{0, 1\}$ for each $i = 1:N$.\\

Logistic regression is a \textbf{probabilistic discriminative classifier}.
In the binary case models the probability as a posterior probability:
$$
P(Y = 0 | x) = \frac{e^{w^T x + b}}{1 + e^{w^T x + b}} \qquad
P(Y = 1 | x) = \frac{1}{1 + e^{w^T x + b}}
$$

Tip: logistic regression probability formulas can be obtained modeling log-odds equal to the value of $w^T x + b$.

\obsbox{
The probability of a point being one and its probability being zero must sum to one,
since the probability of all possible outcomes of an event must sum to one by definition.
}

The classifier is proven linear by computing the decision boundary --- $P(0|x) = P(1|x)$ ---,
passing to the logarithm (equivalent equation, since the logarithm's input is positive by definition, given that it is a probability)
--- $\log P(0|x) = \log P(1|x)$ ---, then subtracting the right-hand-side from the left-hand-side:
\begin{align*}
\log P(0|x) - \log P(1|x)\\
&= \log \frac{e^{w^T x + b}}{1 + e^{w^T x + b}} - \log \frac{1}{1 + e^{w^T x + b}}\\
&= \log e^{w^T x + b} - \log (1 + e^{w^T x + b}) - [\log 1 - \log (1 +e^{w^T x + b})]\\
&= \log e^{w^T x + b} - \log (1 + e^{w^T x + b}) - \log 1 + \log (1 +e^{w^T x + b})\\
&= \log e^{w^T x + b} = w^T x + b
\end{align*}
The geometric locus that satisfies the original equation is the hyperplane (or decision boundary);
so we demonstrated that it is linear.

Logreg can be extended to the multiclass case (\textbf{multinomial logistic regression}) with K classes:
$$
P(Y = c | x) = \frac{e^{w_c^T x + b_c}}{1 + \sum_{l=1}^{K-1} e^{w_l^T x + b_l}} \qquad
P(Y = K | x) = \frac{1}{1 + \sum_{l=1}^{K-1} e^{w_l^T x + b_l}} \qquad
$$

The first probability is used for classes (also called levels, hence $l$) from $1$ to $K-1$.
The second probability is used for the last level, revealing a different treatment for it;
this is due to the fact that the last level is traditionally considered the \textbf{reference level}.

Inference is easy: it just returns the class that gives the maximum probability between the probability of being zero
and the probability of being one.
In the general case, the class that returns the highest probability is selected as follows:
$f_{LR}(x) = \arg \max_{c \in Y} P(Y=c | x)$.

Training aims at finding the best vector $w$ of weights and bias $b$, grouped together under the variable $\theta = (w_c, b_c) \quad c \in Y$.
The parameters are selected to optimize conditional \textbf{log likelihood} over the dataset $D = \{(x_i, y_i), i=1:N\}$:
$$ \theta_* = \arg \max_\theta \mathcal{L}(\theta | D) = \arg \max_\theta \sum_{i=1}^N \log P(Y=y_i | X=x_i)$$
No closed formula exists to solve it. A numerical optimization method is required.

Slide 8: beware, the probabilities have been swapped.

In the binary case, the conditional log likelihood becomes:
$$
\mathcal{L}(\theta|D) =
\sum_{i=1}^N [
  \underbrace{y_i \log P(y=1|x)}_{\text{active if } y_i=1} +
  \underbrace{(1-y_i) \log P(y=0|x)}_{\text{active if } y_i=0}
]
$$
$y_i$ and $(1-y_i)$ coefficients are just a trick to select
they right log probability exploiting the class $y_i \in {0,1}$ of the $i$-th datapoint.

For simplicity, we will omit the bias $b$ from the formulation;
note that the bias can be reintroduced implicitly by passing to a homogenous transformation $w^T x_i$.

The log likelihood formula can be further developed:
\begin{align}
\mathcal{L}(w|D)\nonumber\\
&= \sum_{i=1}^N \{y_i [\log e^{w^T x_i} - \log (1 + e^{w^T x_i})] + (1-y_i) [\log 1 - \log (1 + e^{w^T x_i})]\}\nonumber\\
&= \sum_{i=1}^N \{y_i [w^T x_i - \log (1 + e^{w^T x_i})] + (1-y_i) [- \log (1 + e^{w^T x_i})]\}\nonumber\\
&= \sum_{i=1}^N \{y_i w^T x_i - y_i \log (1 + e^{w^T x_i}) - \log (1 +e^{w^T x_i}) + y_i \log (1 + e^{w^T x_i})\}\nonumber\\
&= \sum_{i=1}^N \{y_i w^T x_i - \log (1 + e^{w^T x_i})\} \label{eq:unrolled_likelihood}
\end{align}

\defbox{
The log likelihood is also called \textbf{negative binary cross-entropy}.
}

We can define function $F(x_i, w) = \sigma(w^T x_i)$, where $\sigma(x) = \frac{1}{1 + e^-x} = \frac{e^x}{1 + e^x}$ is
the logistic function. The logistic function is a sigmoid, i.e., a S-shaped function.
The sigmoid is the smooth verison of a \textbf{step function}.
Every formula above can be rewritten as a function of $F$.
Especially important is the likelihood as a function of the sigmoid function:
$$
\mathcal{L}(w|D) = \sum_{i=1}^N [y_i \log F(x_i, w) + (1-y_i) \log (1 - F(x_i, w))]
$$

Now, we can say that the training has the objective to maximize the log likelihood over the dataset D, or,
analogously, to minimize (solvers generally \textbf{minimize} the loss) the binary cross-entropy.

\subsection{Gradient Descent}
Since we do not have a closed form solution for the optimizaition problem, we use an iterative strategy, a.k.a., gradient descent.
The start point is picked randomly.
The gradient descent update is expressed as: $\theta_j = \theta_j - \alpha \frac{\delta f(\theta)}{\delta \theta_j}$.
The new candidate point is the old minus the derivative (gradient in multivariate case) of the function in the old point, scaled
by the $\alpha$ coefficient.

We stop when the gradient is roughly zero, which is an euristic approach, since there are no guarantees about the global minimum;
the only exception is for convex functions, since they do guarantee that the local and global minimum coincide.

Alpha ($\alpha$) is called leraning rate, which influences the speed of convergence.
Big alphas can be problematic: they could create oscilations around the minimum.
Too small alphas slow down convergence.

Given the loss function, equal to the negative likelihood, $\mathcal{L}(w) = - \mathcal{L}(w|D)$,
we want to take the derivative (actually, the gradient) of $\mathcal{L}(w)$ with respect to $w$.
See \cref{eq:unrolled_likelihood} for the unrolled formula of the negative log likelihood:
$\mathcal{L}(w) = - \frac{1}{N} \sum_{i=1}^N [y_i w^T x_i - \log (1 + e^{w^T x_i})]$.
The $\frac{1}{N}$ can be omitted as we have done so far, since it is just a coefficient that does not
change the optimization problem; however, this will be important in the minibatch approach, since each
minibatch could have a different number of datapoints.\\
Then we compute the partial derivative w.r.t. to each component of $w$:
$$
\frac{\partial{\mathcal{L}(w)}}{\partial{w_j}}
= - \frac{1}{N} \sum_{i=1}^N [ y_i x_i^{(j)} - \frac{e^{w^T x_i}}{1 + e^{w^T \cdot x_i}} x_i^{(j)} ]
= - \frac{1}{N} \sum_{i=1}^N [ (y_i - \sigma(w^T \cdot x_i)) ] x_i^{(j)}
$$

The fist addend is: a constant ($y_i$) by the dot product between vectors $w^T$ and $x_i$ .
Each element of $x_i$ is constant and multiplied with the corresponding element of $w^T$.
Since $w_j$ is the derivation variable, only the j-th element of $x_i$ will survive (since multiplied with the variable $w^T$), while the others
will become zeroes (since they are constant).
The second addend uses the chain rule; it's the composition of log, a sum (constant plus exp) and the product $w^T \cdot x_i$ (same as before).

The derivative of $\mathcal{L}(w)$ w.r.t $w$ is a vector (the gradient), in which the j-th element is the derivative of $\mathcal{L}(w)$ wrt to $w_j$.
During the gradient descent, the update of each weight is done according to the j-th element of the gradient vector:
\begin{align*}
\frac{\partial{\mathcal{L}(w)}}{\partial{w}} =  \begin{bmatrix}
                                                \frac{\partial{\mathcal{L}(w)}}{\partial{w_1}} \\
                                                \frac{\partial{\mathcal{L}(w)}}{\partial{w_2}} \\
                                                \vdots \\
                                                \frac{\partial{\mathcal{L}(w)}}{\partial{w_m}}
                                                \end{bmatrix}\\
w_j = w_j - \alpha \frac{\partial{\mathcal{L}(w)}}{\partial{w}}
\end{align*}

The cost function used in gradient descent is called objective or loss function, and it is typically denoted with $\mathcal{L}(\cdot)$ or $J(\cdot)$.

This is the vanilla rule: \textbf{batch gradient descent}.
It is called batch since we have to run the average $\frac{1}{N} \sum_{i=1}^N [\dots]$ through all $N$ training examples.
Several optimizers are built on top of it to speed up convergence.

A different strategy is \textbf{minibatch stochastic gradient descent}.
The minibatch is a subset of the dataset chosen by randomly sampling the examples.
This way, we can avoid computing the average over all the elements.

In the extreme case, just one random example is sampled from the training set to perform the update.
This is called \textbf{stochastic gradient descent}.
This time, no average is needed: speed is traded with stability.

Full batch gradient descent is best one, minibatch is in the middle, the worst is the fully stochastic version.
The same order holds for speed, from slowest to quickest.

To avoid being stuck at local minimums and traveling zig-zags with SGD, we use strategies like \textbf{ADAM} -- Adaptive Moment Estimation.
The \textbf{momentum} smooths out the path towards the minimum with a \textbf{weighted moving average} of past gradient:
the older the gradient the lower the weight.
It combines momentum with an \textbf{adaptive learning rate} to slow down descent while approaching the minimum (magnitude of the gradient is used to estimate it).

ADAM update rules. $g_t$ denotes the gradient of the loss function w.r.t the vector of weights:
\begin{itemize}
\item \textbf{First moment}: the moving of the average of the gradient is $m_t = \beta_1 m_{t-1} + (1 - \beta1) g_t$. $\beta_1$ is the weight given to past observations.
\item \textbf{Second moment}: the "variance" of the gradient is $v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$. $\beta_2$ is the weight given to past observations.
\item \textbf{Bias correction} for $m_t$: $\hat m_t = \frac{m_t}{1-\beta_1^t}$. Remember that the weighting coeff, $\beta_1$, is below one, converging to zero when raised to infinity.
\item \textbf{Bias correction} for $v_t$: $\hat v_t = \frac{v_t}{1-\beta_2^t}$. Remember that the weighting coeff, $\beta_2$, is below one, converging to zero when raised to infinity.
\end{itemize}

Bias corrections is low with few samples, and it increases as the number of sample becomes higher and higher.
Basically, it prevents the initial estimates to be biased towards zero (their initialization value).

Intuitively: if the path towards the minimum is straight, we can speed up learning rate; if zig-zags (high path variance), slow it.

\textbf{Weight update} is thus corrected as follows: $\theta_{t+1} = \theta_{t} - \alpha \frac{\hat m_t}{\sqrt{\hat v_t} + \epsilon}$

Pros: fast convergence, works well with minimal tuning, handles noisy gradients.\\
Cons: doesn't work well with some models, e.g. generative ones. Requires extra memory for momentum estimate.\\
We add a memory proportional to the number of parameters: two momentum estimates (first and second moment) for each $\theta_i$.