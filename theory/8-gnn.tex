\section{Graph Neural Network}
\subsection{New viewpoint}
CNNs usually operate on 2D matrices, typically images. We can view images as 2D discrete lattices.
Lattices are regular structures with strong locality.
In the same way, we can view timeseries as 1D lattices.

On the contrary, Transfomers do not exploit locality, attention goes beyond.

What is a graph? Set of vertices and edges, in which each edge is a pair of vertices, G = (V, E); can be weighted or unweighted, directed
or undirected, etc. Go review first year's notes.

Applications: biology, social networks, streets, knowledge graphs, and skeletons are examples. \\
Objective: define a NN that can handle graph-structured data. \\

Irregular structure: nodes with arbitrary number of nodes. Variable receptive field. \\
No spatial locality: no inherent notion of distance or order. \\
Permutation invariant: output should not depend on the processing order. \\
No global coordinate system: no fixed layout orientation. \\
Dynamic topology: topology may evolve over time. \\

\subsection{Graph Convolutional Network}
GCNs were invented by Kipf and Welling in 2017.

The basic idea is that every node keeps an embedding, that can be updated by the embeddings of the neighbors.

The key idea is \textbf{message passing}: information is propagated across the graph; information flows according to the topology,
therefore each node can be influenced just by its neighbors. \\
Each node performs \textbf{aggregation}, consisting in aggregating information from neighbors to update its own embedding.

Input:
 - node features matrix X \in R^{n by d}: each row corresponds to the feature vector of a node
 - adjacency matrix A \in R^{n by n}: encodes graph structure and is usually preserved across layers, can be binary
 
 
Adjacency Matrix A. Key component GCCs. Acts as a structural \textbf{prior} about the domain.
Injects prior knowledge informing the model about physics, joint constraints, etc.
Reduces overfitting by enforcing locality and limiting message passing to meaningful connections.

Adj mat can be built with different techniques: domain knowledge, euristics (like thresholding or kNN).

Matrix multiplication can be used to implement message passing. Given H = A @ X, each row of the H matrix corresponds to the X elements
selected by the A matrix.

$\cap A = A + I$ is the adj matrix with enforced self loops. \\
$H^{(l-1)}$ is $H$ from previous layer. \\
$D^{-\frac{1}{2}}$ applied on the left and on the right is a normalization. \\
$\sigma$ is the sigmoid activation function, we can use the activation function we want. \\
$\cap A X W$ is the weigthed aggregation from the neighbors. \\

Recap: a GCN layer performs aggregation over first order neighbors (nodes directly connected), if we want to capture correlations with
higher order neighbors we can compose two of the above fuctions.

TODO: formula

Usages:
 - link prediction: given the connections of the current month, which users will be connected the most next month?
 - community detection for communities clustering and segmentation
 - anomaly detection: why is a node connected to the network when i didn't expect it to be?

\subsubsection{Settings and Tasks}
Node classification scenarios:
 1. trasductive setting. We have one graph with both labeled and unlabeled nodes, we want to infer missing labels from the ones we have. \\
 2. inductive settings. We have many graphs in training phase with all nodes labeled, we want label all nodes of a new graph. \\

Tasks:
 1. link prediction: Predict whether an edge exists between two nodes.
 2. graph classification: predict a label from entire graph.

Link prediction. TODO

Graph classification. \\
Problem: we want to aggregate information from all nodes and get just one output as output. \\
Easy solution: averaging is as always the easiest method to aggregate embeddings in one supermode. \\
We could apply global pooling to all vertices: $h_{global} = pooling(h_v | v \in V)$.

Other methods such as hierarchical pooling there exist: clusters are created in the graph, then pooling is applied.

\subsubsection{Limitations}
Oversmoothing: nodes representaiotn becomes indistinguishable as the number of layers increases

TODO: complete

\subsection{Graph Attentions Networks}
GATs have been introduced to overcome limitations of GCNs.
In this model each node is connected to the others. The importance of each link (its weight) is learnt by the network.

Step 1. Apply a shared linear transformation to all node features. h_i^W = W h_i, where W \in R^{F' by F} is a learnable weights matrix.
Step 2. Compute unormalized affinity score (attention) between each pair of nodes i-j. $LeakyReLU(a^T \[h_i || h_j\])$, where $||$ is vector
concatenation and $a$ is learnable weigth vector.
Step 3. Compute normalized attention scores over neighbors of i. TODO formula
Step 4. Compute final embedding $h_i$ as weighted sum of h values against normalization coefficients. TODO formula

Beware: in GATs adjacency matrix can be used to turn off a link, basically enforcing a zero weighted link. This is called \textbf{masked attention}.

Residual connections can be used in graphs as well. TODO how?

TODO real world applications
