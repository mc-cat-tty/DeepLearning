\section{Recurrent Neural Networks}
\subsection{Usage}
In this lecture we will study NNs for \textbf{time-series} analysis, a.k.a.\ an ordered sequence of observations.

\defbox{A time-series (TS) is a set of observations $\{ x_t \}_{t \in \tau}$ collected over time period $\tau$.}

We will consider discrete and regularly sampled time series, meaning that the time delta (sampling period) between data points is fixed, which
also implies a fixed sampling frequency $f_c$.

Most important features that can be extracted from a time series are:
\begin{itemize}
  \item \textbf{Trend}: if the TS is averagely increasing or decreasing.
  \item \textbf{Seasonality}: regular fluctuations for a given period of reference.
  \item \textbf{Cycle}: periodic fluctuation around the trend.
  \item \textbf{Outliers}: out-of-distribution values w.r.t. the rest of the data.
  \item \textbf{Stationariety}: a TS is stationary if statistics, such as mean and variance, do not change over time.
\end{itemize}

The \textbf{STL} decomposition divides the TS into Seasonality and Trend using the Loess regression.
After STL decomposition we can put together again the trend and seasonality provided by our model and subtract it from the original
TS to get the residual. The \textbf{residual} (basically \textbf{noise}) is made up of irregularities that cannot be decomposed.

\subsection{Forecasting}
Given a period of observation $\tau$ we want to predict the future values of the process $\hat x_{\tau + h}, \ h > 0$.
$h$ is called \textbf{forecast horizon}.

Objective: approximate the \textbf{Data Generating Process} (DGP) with a \textbf{Model} as best as we can:\\
\begin{center}
\begin{BVerbatim}
 DGP --------------> Real-world Evidence
  |                           ^
  |                           |
  -------> Our Model ----------
\end{BVerbatim}
\end{center}

We want a model that matches as much as possible the DGP, to improve forecasting precision;
it boils down to accurately estimate the hidden parameters of the DGP.

Forecasting models typically consider a limited window of observations.
Let $t$ be the current timepoint and $q$ be the window size,
let $\phi$ and $\theta$ be the parameters of the model to be estimated;
given $\{ x_t, \dots, x_{t-q} \}$ observations and $\{ \xi_t, \dots, \xi_{t-q}  \}$ white noise with mean 0 and finite variance
(this noise can be sampled from a given distrubiton, such as a normal distribution with mean zero), our forecasting model prediction has the form:
$$
x_t = f(x_{t-1}, \dots, x_{t-q}; \phi) + g(\xi_{t-1}, \dots, \xi_{t-q}; \theta)
$$

\subsection{Classic Approaches}
\defbox{
  An \textbf{autoregressive model} of order $p$, denoted as AR($p$), is a forecasting model whose output depends on $p$ past samples.
  Is defined as:
  $$
  x_t = \sum_{i=1}^p \phi_i x_{t-i} + \xi_t
  $$
  Where $\phi$ are the learnable parameters of the function, and $\xi$ is the white noise.
}

\defbox{
  A \textbf{moving average model} of order $q$, denoted as MA($q$), is defined as:
  $$
  x_t = \mu + \xi_t + \sum_{i=1}^q \theta_i \xi_{t-i}
  $$
  Where $\mu$ is the empirical average of the process variable up to time $t$, $\theta$ are learnable params of the function, and $\xi$
  are the \textbf{error terms} (or \textbf{shocks}) from previous predictions.
}
This model relies just on the structure of the error term to make a prediction at the current timepoint.

\subsection{Data Pre-processing}
TSs usually live in the time domain.
Sometimes is useful to adopt ``data preparation'' techinques: extract or separate time information, format and refine values.
Other pre-processing techniques are Fourier (pass to the frequency domain) and wavelet transforms.

How to deal with different data types?\\
String-formatted dates (e.g. \textit{2025-01-03}) must be converted to numerical values, like a timestamp,
or split them into year/month/day for more efficient grouping.

Also, \textbf{periodicity} should be preserved, since NNs work better when their inputs are zero-mean and fixed variance.
However, periodic values such as months (from 0 to 12 and then back to 0) days (0 to 31), and hours span a different range of values,
it is then preferable to pass them through $\sin$ or $\cos$ functions to map them to the unit circle.
$\tau$ must be chosen depending on the periodicity of the value; for instance:
$$
\text{hour\_sin} = \sin(\text{hour} \cdot \frac{2 \pi}{24}) \quad
\text{hour\_cos}= \cos(\text{hour} \cdot \frac{2 \pi}{24})
$$

\obsbox{Both sine and cosine are needed to determine a point on the unit circle.}

\subsection{RNN}
RNNs are a framework: they define a family of models that are not just feedforward, like the ones we have seen so far,
meaning that their topology has some feedback connections through which model outputs are fed back into the model itself.
RNNs are \textbf{specialized in processing sequences}.

Different types of RNN architectures exist; they can accept a single-valued input or a sequence of input values, and the same holds
for the output, therefore 4 cardinality combinations exist:
one-to-one, one-to-many, many-to-one, and many-to-many.

RNNs are composed of multiple recurrent cells.

The \textbf{Vanilla RNN} is parametrized over three sets of learnable parameters:
$U$ maps inputs to the hidden state, $W$ models hidden (or latent) state transition, and $V$ maps hidden state to output.
Basically is a composition of three linear models (MLPs) that go from input $x$ to hidden state $h$, from $h$ to output $o$, and from $h^{(t-1)}$ to $h^{(t)}$.

% TODO diagram.

The system dynamics (update formulas) are:
\begin{equation}
  \begin{cases}
    h^{(t)} = \phi(W h^{(t-1)} + U x^{(t)})\\
    o^{(t)} = V h^{(t)}
  \end{cases}
\end{equation}

The hidden state keeps memory of what happened in the past, a.k.a a compressed (lossy) state of the sequence of past inputs.
The ``summary'' stored in the hidden state is necessarily lossy, since it maps an arbitrary length input $(x^{(1)}, \dots, x^{(t)})$
to a fixed vector $h^{(t)}$.

% TODO unrolled diagram.

A recurrent architecture can be unfolded into a Direct Acyclic Graph (DAG), from a mathematical point of view:
$h^{(t)} = f(h^{(t-1)}, x^{(t)}; \theta)$

Where $\theta$ are learnable params, $h^{(t)}$ is the hidden state at time $t$ and $x^{(t)}$ the input at time $t$.

\subsection{Backpropagation}
Notice that hidden layer weights are shared between time points, but we can still train the model through
backpropagation unfolding the recurrent graph.
Since the gradient conceptually flows back in time, we talk about \textbf{Backpropagation Through Time} (BTT).

Computing BTT involves computing a huge number of derivatives. We want to update all the three weights matrices
$V$, $W$, and $U$, taking into consideration that the backprop signal received by the $W$ and $U$ matrices depend
not only on the current state, but also on the sequence of previous states.
Mathematically, each timepoint gives a contribution to the update of both the matrices $W$ and $U$.
The compact version of the gradients are:
\begin{align*}
  \frac{\partial{L}}{\partial{V}} &= \frac{\partial{L}}{\partial{o}} \frac{\partial{o}}{\partial{V}}\\
  \frac{\partial{L}}{\partial{W}} &= \frac{\partial{L}}{\partial{o}} \frac{\partial{o}}{\partial{h^{(3)}}} \left[ \sum_{k=1}^3 \left( \frac{\partial{h^{(3)}}}{\partial{h^{(k)}}} \frac{\partial{h^{(k)}}}{\partial{W}} \right) \right]\\
  \frac{\partial{L}}{\partial{U}} &= \frac{\partial{L}}{\partial{o}} \frac{\partial{o}}{\partial{h^{(3)}}} \left[ \sum_{k=1}^3 \left( \frac{\partial{h^{(3)}}}{\partial{h^{(k)}}} \frac{\partial{h^{(k)}}}{\partial{U}} \right) \right]\\
\end{align*}

Square brackets contain contributions of all the timepoints to $\frac{\partial{L}}{\partial{W}}$ and $\frac{\partial{L}}{\partial{U}}$.

\obsbox{
  $\frac{\partial{L}}{\partial{V}}$ depends only on the current state; while $\frac{\partial{L}}{\partial{W}}$ and $\frac{\partial{L}}{\partial{U}}$ depend on all the previous states.
}

Clearly, the expanded versions of these formulas take also into account that
$\frac{\partial{h^{(t)}}}{\partial{h^{(k)}}}$ cannot be always computed directly; it needs, in turn, to be expanded with the chain rule.
For instance, $\frac{\partial{h^{(3)}}}{\partial{h^{(1)}}}$ becomes $\frac{\partial{h^{(3)}}}{\partial{h^{(2)}}} \frac{\partial{h^{(2)}}}{\partial{h^{(1)}}}$.

\proofbox{
As the time series progress, this chain of derivatives becomes longer and longer:
\begin{enumerate}
  \item let $h^{(t)} = W h^{(t-1)} + U x^{(t)}$ be the recurrence relation under the assumption that $\phi$ is linear
  \item the derivative w.r.t.\ the previous timestep is $\frac{\partial h^{(t)}}{\partial h^{(t-1)}} = \frac{\partial (W h^{(t-1)} + U x^{(t)})}{\partial h^{(t-1)}} = W$
  \item applying the chain rule to correlate the dependency between the (arbitrarily chosen) timestep 4 and timestep 1, we get $\frac{\partial h^{(4)}}{\partial h^{(1)}} = \frac{\partial h^{(4)}}{\partial h^{(3)}} \cdot \frac{\partial h^{(3)}}{\partial h^{(2)}} \cdot \frac{\partial h^{(2)}}{\partial h^{(1)}}$
  \item which becomes $\frac{\partial h^{(4)}}{\partial h^{(1)}} = W^3$
\end{enumerate}
}

% TODO: d h3 in d h0, delete ReLU (make assumption of being in linear part, which copies the function), result W to the pow of 3

Problem: if the magnitude (measured with L1 or L2 norm) of the parameters is \textbf{below one}, when elevated to some power, the gradient will approach zero.
This is called \textbf{vanishing gradient}.
If params are \textbf{greater than one}, when elevated to some power, the gradient will be amplified.
This is called \textbf{exploding gradient}.

Vanishing gradient is problematic, because contributions of older timesteps approach zero as the sequence progresses. The oppsite happens with exploding
gradient: older timesteps contributions become bigger and bigger as the sequence progresses. This numerical instability is typically associated with the RNNs,
and with deep NNs in general; in fact, even feedforward networks can suffer vanishing/exploding gradients, but in RNNs the problem is amplified by
the fact that the network is as deep as the length of the input sequence.

Long-term dependencies are the problem. We would like to check numbers and increase them if they are too low and decrease them if too high.
But we cannot inject random values into recurrent cells\dots

\subsection{LSTM}
\textbf{Gating} is the first mitigation strategy: instead of overwriting the hidden state at each timestep, each element of the hidden state
matrix is passed through a learnable gating coefficient. The gating coefficients control the information flow, dedicing whether to drop or keep
any piece of information that determines the current state.

\textbf{Gated Recurrent Unit} (\textbf{GRU}) and \textbf{Long Short-Term Memory} (\textbf{LSTM}) architectures implement gating to mitigate
the vanishing/exploding gradient problem and allow the network to learn long-term dependencies.

% TODO diagram.

LSTM introduces the cell state $C$, which constitues the \textbf{long-term memory} of the model; which is added to the already existing hidden state $h$,
which acts as the \textbf{short-term memory} of the model.
LSTM substitutes the recurrent cell with the \textbf{LSTM Memory Cell}, which has as inputs the short-term hidden state $h^{(t-1)}$, the long-term cell state
$C^{(t-1)}$, and the current input $x^{(t)}$, and as outputs the short-term hidden state $h^{(t)}$ and the long-term cell state $C^{(t)}$.

This architecture is composed of three gates, working on the same dimension of the hidden space:
\begin{itemize}
  \item \textbf{forget gate}: $f = \phi(W_f h^{(t-1)} + U_f x^{(t)})$. Later multiplied with the previous cell state $C^{(t-1)}$, selects which and how much
  information can be overwritten from $C^{(t-1)}$. The sigmoid squashes the values between 0 and 1, where 0 represents ``forget'' and 1 represents
  ``keep''. This equation basically outputs a ``mask'' (not binary since real values between 0 and 1 are allowed).
  \item \textbf{input gate}: $i = \phi(W_i h^{(t-1)} + U_i x^{(t)})$. Later multiplied with the ``mask'' $\tilde{C}$, the input gate controls how much of the
  previous hidden state let through. Intuitively, it selects the important information that must be preserved from the input.
  \item \textbf{output gate}: $o = \phi(W_o h^{(t-1)} + U_o x^{(t)})$. Similarly to the input, the output gate controls how much of the previous hidden state $h^{(t-1)}$ must be expose to the
  next LSTM cell through the output $h_t$.
\end{itemize}

The above-mentioned $\tilde{C}$ has the following equation: $\tilde{C} = \tanh (W_C h^{(t-1)} + U_C x^{(t)})$
Intuitively, it selects the \textit{candidate} information to be added to the masked $C^{(t-1)}$; the output is only
a candidate since the gate $i$ has the power to remove further information from $h^{(t-1)}$.

Lastly, the outputs are:
\begin{itemize}
  \item \textbf{current cell state}: $C^{(t)} = C^{(t-1)} \odot f + \tilde{C} \odot i$. The masked previous cell state is added to the masked input.
  \item \textbf{current hidden state}: $h^{(t)} = \tanh (C^{(t)}) \odot o$. The amount of information exposed by the output hidden state is regulated by the output gate.
\end{itemize}

\obsbox{
Each gate multiplied element-wise with other functions, acting as continous, differentiable switches (they must be differentiable to be learnable). 
}

This is a summary of the LSTM update equations:
\begin{equation*}
  \begin{cases}
    f = \phi(W_f h^{(t-1)} + U_f x^{(t)})\\
    i = \phi(W_i h^{(t-1)} + U_i x^{(t)})\\
    o = \phi(W_o h^{(t-1)} + U_o x^{(t)})\\
    \tilde{C} = \tanh (W_C h^{(t-1)} + U_C x^{(t)})\\
    C^{(t)} = C^{(t-1)} \odot f + \tilde{C} \odot i\\
    h^{(t)} = \tanh (C^{(t)}) \odot o
  \end{cases}
\end{equation*}

\obsbox{
  Remember that $\sigma$ outputs values from 0 to 1, while $\tanh$ values span form -1 to 1.
}