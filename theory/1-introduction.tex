\section{Introduction}
\subsection{Learning}
Learning theories:
\begin{itemize}
 \item \textbf{Behaviourism} (Skinner): long term change in behaviour due to experience.
 \item \textbf{Cognitivism} (Gestalt school): process of integrating new information in mental frameworks, and updating frameworks.
 \item \textbf{Connectionism} (Hebb): process in which neurons join by developing synapses.
\end{itemize}

\subsection{Machine Learning}
\textbf{Samuel \& Mitchell}: ML gives a computer the ability to learn vs a computer learns from experience wrt a class of tasks
and a performance measure, if performance increases with experience.

\textbf{Supervised vs unsupervised}: learning to predict vs learning to classify and represent.\\
E.g.: classification and regression are supervised, while clustering and dimensionality reduction are unsupervised.

\textbf{Parametric vs non-parametric}: learning involves fitting data to a known model vs learning by ``memorizing'' training dataset.

\subsection{Deep Learning}
\textbf{LeCun}: ML algos that model high-level features in data w/ a composition of non linear transformations.
Not only about classification, the features of the dataset are learned by the model.

\textbf{Timeline}: McCulloch and Pitts neuron ('43), Rosenblatt's perceptron ('58), Minksy and Papert ('69), Rumelhart-Hinton-Williams
backpropagation ('86), LSTM memory ('97), Deep Belief Networks ('06), Goodfellow et al's Generative Adversarial Networks ('14).

\textbf{Gen AI}: bayesian classifier is the simplest generative model.\\
\textbf{Objective} of generative AI: learn a parametric distribution from which you can sample. We will see variational autoencoders.\\
\textbf{See}: geometric feature learning, graph NN, and diffusion models.

\subsection{Exam}
All topics, even formulas. No exercises or programming. Oral interview.