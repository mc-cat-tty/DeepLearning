\section{Introduction}
\subsection{Learning}
Learning theories:
\begin{itemize}
 \item Behaviourism (Skinner): long term change in behaviour due to experience
 \item Cognitivism (Gestalt school): process of integrating new information in mental frameworks, and updating frameworks
 \item Connectionism (Hebb): process in which neurons join by developing synapses.
\end{itemize}

\subsection{Machine Learning}
Samuel \& Mitchell: ML gives a computer the ability to learn vs a computer learns from experience wrt a class of tasks
and a performance measure if performance increases with experience.

Supervised vs unsupervised: learning to predict vs learning to classify and represent.
E.g. classification and regression are supervised, while clustering and dimensionality reduction are unsupervised.

Parametric vs non-parametric: learning involves fitting data to a known model vs learning by memorizing training dataset

\subsection{Deep Learning}
LeCun: ML algos that model high-level features in data w/ a composition of non linear transformations

Not only about classification, the features of the dataset are learned by the model.

Timeline: McCulloch and Pitts neuron ('43), Rosenblatt's perceptron ('58), Minksy and Papert ('69), Rumelhart-Hinton-Williams
backpropagation ('86), LSTM memory ('97), Deep Belief Networks ('06), Goodfellow et al's Generative Adversarial Networks ('14).

Gen AI: bayesian classifier is the simplest generative model.
Objective of generative AI: learn a parametric distribution from which you can sample. We will see encoders and fusion model. 

See: geometric learning, graph NN (where the idea is to model the hyperplane where the features reside in), and diffusion models

\subsection{Exam}
All topics, even formulas. No exercises or programming.

Oral interview.