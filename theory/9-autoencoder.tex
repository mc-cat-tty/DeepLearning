\section{Autoencoders}
\subsection{Intro}
Remember: supervised vs unsupervised learning.
In unsupervised learning features according to which the model group inputs are found autonomously.

Most of the larning techniques give their best in supervised learning setting.s

Then, why unsupervised? \\
First of all, practical reasons: annotating all yt videos would be impractical. \\
Secondly, frequent structure and hidden patterns are already present on the dataset, regardless of supervision signal;
e.g., traits of faces can be inferred by the network after seeing millions or billions of them. \\

Last reason is that unsupervised pre-training offers a init matrix that is not random, helping the network better generalize the task.
It basically consists of giving the model a vague idea of the field is going to work in.

Take for instance the MNIST dataset. All possible input configurations are, given that the dataset contains BW images, 2**274 combinations.
However, there exist a subspace able to contain all digits much smaller than the original one.

Remember: PCA --- Principal Component Analysis --- allows us to find a good representation of data. However, PCA is linear since it
consists in a rototraslation matrix.

A classical unsupervised learning taks is finding a good data representation, that preserves as much as information of original data as
possible respecting the constraints imposed. The new representation must be simpler or with lower dimensionality.

Lower dimentional representations are basically compressions of original data into a smaller representation.

A representation is task specific if, for instance, the model learns to compress just the ones; it follows that the final model
can be used just with ones. If can be applied for whatever digit, then the repr is taks agnostic.

Tip: implement PCA following a tutorial \\
Cons of PCA: linear, TODO: other cons?

\subsection{Autoencoders}
Idea: train a NN with a bottolneck hidden layer and try to make the output match the input. TODO diagram

Def. an autoencoder is a feedforward (aka goes from the input to the output) neural network trained to copy the input to the output
after compressing the input. LHS is called encoder, RHS is called decoder; the hidden layer in the middle is called code.
The code is a projection of input to a manyfold space.

TODO: specific diagram

Why cannot just train that architecture with SGD and backpropagation and end the explaination here? \\
Problem: as always, if overparametrized, the network can learn the identity transformation, avoiding compress data to the code. \\
Solution: we need some form of regularization.

\subsection{Undercomplete Autoencoder}
In undercomplete autoencoder the hidden layer has a smaller dimension than x, forcing the architecture to compress data.
The loss function is, for instance, the MSE and aims to penalize output from being dissimilar to the input. \\

Problem: encoder and decoder can memorize information indexing it in the hidden layer; this overfitting happens if the model is overparametrized.
However, if too much params are removed the network will underfit, making the compression to lossy.

\subsection{Sparse Autoencoder}
Sparse Autoencoder is abbreviated with SAE.

L_SAE = D(x, g(f(x))) + \lambda |h|

The second term is called sparsity penality: enforces the hidden layer to use as few pages as possible in the dictionary.

In this case the normalization term enforces sparsity in hidden layers activations, contrary to the MLP case in which the regularization
uses the L1 norm of the weights.

Sparse autoencoders can discover representations that account for most information relying on partially related concepts.
Autoencoders will partially learn to 'copy' input from output. The hidden layers embeds something called 'learned dictionary', which
is a set of activations that tells the decoder which concepts are encoded in the input.

Autoencoders (specifically the encoder portion) can be used to cluster objects. The resulting code is the cluster name.
Objects that share the same properties will share the same code.

\subsection{Denoising Autoencoder}
A denoiser autoencoder (DAE) has the purpose of cleaning up input data from noise.

L_DAE = D(x, f(g(\cap x)))

The loss is designed so that the original input is reconstructed from a corrupted version of it.

Conceptually, the DAE is able to push the corrupted point \cap x to the right location of the original point x on the manifold.

The loss has't the sparsity penality so the code is dense; as always its shape is a hourglass.

Real life application: denoising autoencoders used for anomaly detection.
The model is trained with data associated with good samples. Then, when fed to an anomalous sample, it will project it to its good
counterpart, amplyfing the error from the original one.

\subsection{Contractive Autoencoder}
Def. a contractive autoencoder (CAE) is an autoencoder that reconstructs the input form the code and makes the code insensitive to the input.s

L_CAE = D(x, g(f(x))) + \lambda |\grad{f(x)}{x}|^2

Note that f(x) is the output of the encoder part, which corresponds to h.
As always we add a regularization term that depends on the code.

It is called contractive since the CAE wraps space such that an input neighborhood is mapped to a smaller output neighborhood.
This loss makes the AE robust to noise, since it "ignores" dimensions that are irrelevant to the compression task.
CAEs are insensitive wrt small variations in the input.

