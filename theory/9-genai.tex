\section{Generative Models}
\subsection{Background}
In this section the concept of \textbf{marginal distribution} is often referenced.

\defbox{
  A \textbf{marginal distribution} is the distribution we obtain aggregating (by summing or integrating)
  the original probability distribution over a subset of the original variables; the resulting
  distribution will no longer be parametrized over the aggregated variables (``ignoring'' them).
}

Intuitively, it consists in flattening a highly dimensional space into a lower dimensional one,
reducing the degrees of freedom of the probability function.

The resulting distribution is called ``marginal'' because in the days of paper tables, mathematicians
used to sum up rows or columns on the margins of a page.

E.g.:\ given the joint probability function $P(S = \text{suit}, N = \text{number}) = \frac{1}{52}$
of drawing a card with the specified suit and number from a deck of 52 poker cards, if we
marginalize out $N$ we get
$P(S = \text{suit}) = \sum_{n \in {2, 3, \dots, K, A}} P(S = \text{suit}, N = n) = \frac{1}{4}$.

\subsection{Objective}
This section is about \textbf{variational autoencoders} (VAEs) and \textbf{generative adversarial networks} (GANs).

Given a dataset, we want to learn the \textbf{probability distribution} $P(x)$ of the datapoints in
the dataset, and \textbf{sample} from that distribution to generate new datapoints.\\
E.g., form a dataset of cat images, learn to generate (\textbf{synthesize}) the image of a new cat.

Two different applications are:
\begin{itemize}
  \item \textbf{Sampling}: sample new example $x ~ P(x)$ from the probability distribution.
  \item \textbf{Inference}: process of computing the probability $P(x)$ of a datapoint $x$ belonging to the distribution.
  This enables anomaly detection.
\end{itemize}

Denoising and contractive autoencoders can implicitly learn the probability distribution of the dataset,

\subsection{VAE}
Stands for \textbf{Variational Autoencoder} (VAE).

Denoising and contractive autoencoders can implicitly learn the probability distribution $P(x)$,
however, it is very difficult to sample new examples from these models.

The simplest generative model is a model that learns $\mathcal{N}(X | \mu_\text{cat}, \ \Sigma_\text{cat})$
for each pixel, and samples from it.
This is an overly simple distribution, plus we are assuming pixels independence.
This generative model operates over the pixel space, which is not optimal: we could keep the simple distribution
assumption and change just the space on which we operate.

\defbox{
  \textbf{Latent variable models} are models in which we relate a set of observable variables to a set of latent variables, so that
  the that we can derive an observable data $x$ from $z$: $z \rightarrow x$.
  Latent variables represent hidden quantities, that can explain the observable data.
}

% TODO: diagram phi, z, x, phi.

Since we cannot observe $z$, the observed data distribution is obtained by marginalizing over $z$:
$$
\underbrace{P(x)}_\text{marginal probability} = \quad
\int \underbrace{P(x, z)}_\text{joint probability} \ \text{d} z \quad
\underbrace{= \int P(x | z) P(z) \ \text{d} z}_\text{from Bayes' theorem}
$$
Explanation: integral of all $z$s conditioned on joint probability $P(x, z) = P(x \cap z)$.

This is an \textbf{intractable} problem, since it is an indefinite integral (upper and lower unbounded), on a continous space.
We move to a discrete space and apply \textbf{Monte Carlo integration} to solve it:
$$
P(x) \approx \frac{1}{K} \sum_{k=1}^K P(x | z^{(k)}), \quad z^{(k)} \sim P(z)
$$

The \textbf{larger} the number of samples $K$, the better the approximation.
Notice that the explicit weighting of the previous formula becomes an implicit weighting; the distribution of the drawings
from $P(z)$ influences the final result through $P(x | z^{(k)})$.
We obviously have to learn $P(x | z)$ and $P(z)$ from the dataset.

Sampling procedure:
\begin{enumerate}
  \item sample $z$ from $P(z)$
  \item sample $x$ from $P(x|z)$
  \item integrate with Monte Carlo
\end{enumerate}

Sampling from the prior distribution $P(z)$ is inefficient: the portion of the distribution from which we sample,
has a huge impact on the final result.
We want to sample from points that are significant for the observation $x$; however, most of the sampled $z$s are
irrelevant to $x$. To get a good estimate of $P(x)$ we need a large number of samples.

Idea: learn a model that, given an observation $x$, predicts the most plausible latent variables $z$. Meaning,
those latent variables that are more related to $x$ under the posterior distribution $P(z|x)$.

Solution: \textbf{variational inference}\\
Idea: modeling the ture posterior distribution $P(z|x)$ with a simpler distribution, the \textbf{proposal distribution} $Q(z|x)$.

The \textbf{proposal distribution} is a simpler, easier to evaluate distribution, from which we sample instead of $P(z)$.
This distribution provides good sampling points, that allows to estimate $z$ with fewer samples.
Instead of sampling $z$ from $P(z)$ (step 1), we sample from $Q(z|x)$.

Clearly, we want $Q(z|x)$ to be as close as possible to the true posterior $P(z|x)$; in other words, we want
their \textbf{divergence}** to be \textbf{as low as possible}:
$$
Q*(z|x) = \arg \min_Q D_\text{KL} \left[ Q(z|x) \| P(z|x) \right]
$$

** $D_{KL}$ is Kullback-Leibler divergence. The analogous of euclidean distance but for distributions:
\begin{align*}
D_{KL}(Q(z|x) \| P(z|x))& \\
& = \mathbb{E}_{x \sim Q(z|x)} \left[ \log \frac{Q(z|x)}{P(z|x)} \right] \\
&= \mathbb{E}_{x \sim Q(z|x)} \left[ \log Q(z|x) - \log P(z|x) \right]
\end{align*}

Under the hood, the KL divergence is just the expected value (expectation taken using probability $P$) of the logarithmic
difference between probabilities $Q$ and $P$.

% $$
% ... = E_{z \approx q(z|x)}[log q(z|x) - log p(z|x)] = ...
% $$

% From Bayes:
% $$
% p(z|x) = p(x \cup z) / p(x)
% p(x|z) = p(x \cup z) / p(z)
% substituting
% p(z|x) = ( p(x|z) p(z) ) / p(x)
% $$

% Putting it into the previous formula:
% $$
% ... = E_{z \approx q(z|x)} [log q(z|x) - log [( p(x|z) p(z) ) / p(x)]]
% = E_{z \approx q(z|x)} [log q(z|x) - [ log p(x|z) + log p(z) - log p(x)] ] // Log properties
% = E_{z \approx q(z|x)} [log q(z|x) - log p(x|z) - log p(z) + log p(x) ]  // Consuming the minus
% = E_{z \approx q(z|x)} [log q(z|x) - log p(x|z) - log p(z)] + E_{z \approx q(z|x)} log p(x)  // Exploiting expected value linearity
% = E_{z \approx q(z|x)} [log q(z|x) - log p(x|z) - log p(z)] + log p(x)  // But p(x) is indepndent from z
% = E_{z \approx q(z|x)} [log q(z|x) - log p(z)] - E_{z \approx p(x|z)} [log p(x|z)] + log p(x)  // Rearranging and braking terms inside E
% $$

% The first addend is now KL divergence. The final formula is:
% $$
% ... = KL (log q(z|x) || log p(z)) - E_{z \approx p(x|z)} [log p(x|z)] + log p(x)
% $$

% $$
% D_{KL}[q(z|x) || p(z|x)] = KL (log q(z|x) || log p(z)) - E_{z \approx p(x|z)} [log p(x|z)] + log p(x)
% D_{KL}[q(z|x) || p(z|x)] - log p(x) = KL (log q(z|x) || log p(z)) - E_{z \approx p(x|z)} [log p(x|z)]
% log p(x) - D_{KL}[q(z|x) || p(z|x)] = E_{z \approx p(x|z)} [log p(x|z)] - KL (log q(z|x) || log p(z))
% $$

% The second term is the loss function of the variational autoenc and is called ELBO.

% Analyzing the terms:
%  - log p(x) is fixed gieven the dataset
%  - D_{KL} is intriniscally positive since it is a distance

% Therefore, by maximizing the ELBO we are minimizing the D_{KL} term. The name VARIATIONAL autoencoders comes from this step.
% We are therefore minimizing the variation from the two distributions.

% The variational atuoencoeder VAE is an eutoencoder trained to maximize the Evidence Lower Bound ELBO.
% Differently from the standard AE, the variational one does not output a deterministic vector, but the params of a latent distribution.

% TODO diagram

% x -> Encoder -> q_{\phi}(z|x) -> Decoder -> \hat x

% The output is dependend on the distribution on the latent layer.

% Therefore: $E_{z \approx p(x|z)} [log p(x|z)] - KL (log q(z|x) || log p(z))$ can be viewed as two addends the minimiaztion term plus the regularization term.
% p(z) is the prior distribution, we have to choose it. We usually use a normal distribution zero centered and with a parametric variance: $p(z) -> \N (z | \mu = 0, \sum = I)$.
% Therefore the posterior distribution is alike a normal distribution with mean and variance returned by the VAE.

% In practical terms the VAE returns two vectors, representing the mean and the variance.
% x -> f(x; \theta) = [\mu(x), \sigma^2(x)].

% Note that the regularization term acts on the middle of the encoder, while the minimization term on the output.
% We enforce a latent space distribution similar to the gaussian one, without collapsing it to a gaussian, otherwise the process
% wouldn't be invertible.

% How to generate new examples?
% Option one, sampling from the posterior distribution.
% However, if training went well, the posterior Q(z|X) is very similar to the prior. We can sample from the prior.

% The process to generate new images is getting a sample from the prior (a normal distribution), feeding it to the decoder, and getting its output.

% How to make inferece (for instance to perform anomaly detection)?
% From theory:
% $$
% log p(x) - D_{KL}[ q(z|x) || p(z|x) ] = TODO RHS
% $$

% Given that the GL divergence is low, we can say that log p(x) is approsimately the RHS term.

% Enc -> $\mu, \sigma^2$ -> $z \approx N(\mu, \sigma^2)$ -> Dec
% The sampling step is not differentiable, so the previous pipeline is not end to end differentiable.
% We want something that is backpropagation safe: we use the reparametrization trick.
% The idea is to substitute sampling with: $\epsilon \approx N(0, 1), z = \mu + \sigma^2 \epsilon$; sample noise from normal
% zero mean and shift mu according to a fraction of the sigma squared dependent on the epsilon.
% The sampling is an external procedure.

% \subsection{GANs}
% Generative Adversarial Netowrks
% Note the final s: multiple networks will be there.

% Problem: the images generated by VAEs are 

% Differently from VAEs, we have no explicid density function. We learn to map noize z ~ p(z) directly to real data via adversarial training.

% We train the network to transform a noise vector z ~ p(z) sampled from a distribution to some data that look alike the original ones.

% Two actors:
%  - generator: generates data as close as possible to reality
%  - discriminator: given a data from the generator, has the objective to determine if it comes from generator or real dataset, aka if it is real or fake

% During training, generator will learn how to fool the discriminator.
% The generator improves over training, it learns how to generate fake data really similar to real ones.
% At the same time, the discriminator improves. It gets better and better in distinguishing if a data is real or fake.

% This is a minmax problem, since the two players go towards different objectives:
% minmax[ accuracy/likelihood of distriminator on real data + accuracy/likelihood of distriminator on generated data] TODO complete formula
% The second one is the only one where G is present.

% With gradient descent we do not know how to maximaze a quantity. We can put a minus in front of the loss or use gradient ascent,
% which is the same as descent, whith the only difference that there is a plus instead of a minus in front of the gradient.

% Notice that one is derived wrt theta_G and the other one wrt theta_D.
% In PyTorch terms corresponds to different calls to backward().

% If the discriminator is too good at discriminating and the generator is not generating realistic images yet,
% the gradient received by the generator becomes zero and do not improves quality too early.
% We can maximize log(D(G(z))) instead of minimizing 1-log(D(G(z))); the optimization problem is the same, but the gradient signal is higher
% during initial training phases, in which the generator is still not good at generating realistic images.

% TODO trainig algorithm code

% The number of training steps of generator and discriminator can be different.
% The exact number of steps depend on how the generator compares with respect to the discriminator.

% Be aware that in GANs we don't have explicit distribution parameters, contrary to the VAEs.
% We cannot learn the distribution, just learn from it.

% By combining the inputs of the GANs we can obtain combined images from different classes.
% Eg. mixing the input that gives men with glasses, sustracting the input that give men, and adding the input of women; we should obtain
% women with glasses.

% Evolution of GANs.
% A series of further improvements regarding the training stability has been published in literature.

% CycleGAN performs image-to-image translation, specifically, it translate an input image to the equivalent in a target style (eg. Monet painting).
% Up to this paper this kind of networks were trained with paired examples, an image on the input domain X and an image on the output domain Y.
% This work presents an approach for learning translation without paired examples.
% "Cyclic" comes from the fact that an additional constraint is added (otherwise, the problem would have been constrained): the inverse mapping
% Y -> X has to be learned by the network; the loss is in fact called \textit{cyclic consistency} loss.
% Two different adversarial discriminators are then used: D_Y and D_X.
% The cyclic nature of the network is a way of regularazing.

% Problem: GANs provide a fixed resolution output.
% Solution: progressive GANs trains at low resolution and progressively increase resolution up to full scale.
% At each step we increase resoluiton, eg. by doubling size, and retrain the network with an additional layer.
% This way, the network will become capable of generating images at different resolution, even at high ones, without wasting too much 
% memory and computing power. The previously trained layers will give a good starting point to the next training step.
% Nvidia was able to generate 1Mpixels images.

% StyleGANs are style-based genertors. It exploits a style vector plugged into the image with AdaIN.
% AdaIN is basically batch normalization plus a rescale and shift whose parameters Y are given by an MLP:
% adain(Y) = Y_s \frac{x - \mu_B}{\sigma_B} + Y_B.

% The more the styles is disentangled, the more they are composable.
% Disentangled means dot product near zero. If they are not disentangled one style will overwrite the other one.