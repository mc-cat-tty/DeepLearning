\section{Generative Models}
\subsection{Background}
In this section, the concept of a \textbf{marginal distribution} is often referenced.

\defbox{
  A \textbf{marginal distribution} is the distribution we obtain aggregating (by summing or integrating)
  the original probability distribution over a subset of the original variables; the resulting
  distribution will no longer be parametrized over the aggregated variables (``ignoring'' them).
}

Intuitively, it consists in flattening a high-dimensional space into a lower dimensional one,
reducing the degrees of freedom of the probability function.

The resulting distribution is called ``marginal'' because in the days of paper tables, mathematicians
used to sum up rows or columns on the margins of a page.

E.g.:\ given the joint probability function $P(S = \text{suit}, N = \text{number}) = \frac{1}{52}$
of drawing a card with the specified suit and number from a deck of 52 poker cards, if we
marginalize out $N$ we get
$P(S = \text{suit}) = \sum_{n \in {2, 3, \dots, K, A}} P(S = \text{suit}, N = n) = \frac{1}{4}$.

\subsection{Objective}
This section is about \textbf{variational autoencoders} (VAEs) and \textbf{generative adversarial networks} (GANs).

Given a dataset, we want to learn the \textbf{probability distribution} $P(x)$ of the datapoints in
the dataset, and \textbf{sample} from that distribution to generate new datapoints.\\
E.g., form a dataset of cat images, learn to generate (\textbf{synthesize}) the image of a new cat.

Two different applications are:
\begin{itemize}
  \item \textbf{Sampling}: sample new example $x \sim P(x)$ from the probability distribution.
  \item \textbf{Inference}: process of computing the probability $P(x)$ of a datapoint $x$ belonging to the distribution.
  This enables anomaly detection.
\end{itemize}

Denoising and contractive autoencoders can implicitly learn the probability distribution of the dataset,

\subsection{VAE}
Stands for \textbf{Variational Autoencoder} (VAE).

Denoising and contractive autoencoders can implicitly learn the probability distribution $P(x)$,
however, it is very difficult to sample new examples from these models.

Given the task of cat images generation,
the simplest generative model is a model that learns $\mathcal{N}(X \ | \ \mu_\text{cat}, \Sigma_\text{cat})$
for each pixel, and samples from it.
This is an overly simple distribution, plus we are assuming pixels independence.
This generative model operates over the pixel space (the same space of the observation in which the observation lives),
which is not optimal: we could keep the simple distribution
assumption and change just the space on which we operate.

\defbox{
  \textbf{Latent variable models} are models in which we relate a set of observable variables to a set of latent variables, so that
  the that we can derive an observable data $x$ from $z$: $z \rightarrow x$.
  Latent variables represent hidden quantities, that can explain the observable data.
}

% TODO: diagram phi, z, x, phi.

Since we cannot observe $z$, the observed data distribution is obtained by marginalizing over $z$:
$$
\underbrace{P(x)}_\text{marginal probability} = \quad
\int \underbrace{P(x, z)}_\text{joint probability} \ \text{d} z \quad
\underbrace{= \int P(x | z) P(z) \ \text{d} z}_\text{from Bayes' theorem}
$$
Explanation: integral of all $z$s conditioned on joint probability $P(x, z) = P(x \cap z)$.

This is an \textbf{intractable} problem, since it is an indefinite integral (upper and lower unbounded), on a continous space.
We move to a discrete space and apply \textbf{Monte Carlo integration} to solve it:
$$
P(x) \approx \frac{1}{K} \sum_{k=1}^K P(x | z^{(k)}), \quad z^{(k)} \sim P(z)
$$

The \textbf{larger} the number of samples $K$, the better the approximation.
Notice that the explicit weighting of the previous formula becomes an implicit weighting; the distribution of the drawings
from $P(z)$ influences the final result through $P(x | z^{(k)})$.
We obviously have to learn $P(x | z)$ and $P(z)$ from the dataset.

Sampling procedure:
\begin{enumerate}
  \item sample $z$ from $P(z)$
  \item sample $x$ from $P(x|z)$
  \item integrate with Monte Carlo
\end{enumerate}

Sampling from the prior distribution $P(z)$ is inefficient: the portion of the distribution from which we sample,
has a huge impact on the final result.
We want to sample from points that are significant for the observation $x$; however, most of the sampled $z$s are
irrelevant to $x$. To get a good estimate of $P(x)$ we would need a large number of samples (inefficiency).

Idea: learn a model that, given an observation $x$, predicts the most plausible latent variables $z$. Meaning,
those latent variables that are more related to $x$ under the posterior distribution $P(z|x)$.

Solution: \textbf{variational inference}\\
Idea: modeling the ture posterior distribution $P(z|x)$ with a simpler distribution, the \textbf{proposal distribution} $Q(z|x)$.

The \textbf{proposal distribution} is a simpler, easier to evaluate distribution, from which we sample instead of $P(z)$.
This distribution provides good sampling points, that allows to estimate $z$ with fewer samples.
Instead of sampling $z$ from $P(z)$ (step 1), we sample from $Q(z|x)$.

Clearly, we want $Q(z|x)$ to be as close as possible to the true posterior $P(z|x)$; in other words, we want
their \textbf{divergence}** to be \textbf{as low as possible}:
$$
Q^*(z|x) = \arg \min_Q D_\text{KL} \left[ Q(z|x) \| P(z|x) \right]
$$

** $D_\text{KL}$ is Kullback-Leibler divergence. The analogous of euclidean distance but for distributions:
\begin{align*}
D_\text{KL} \left[ Q(z|x) \| P(z|x) \right]
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log \frac{Q(z|x)}{P(z|x)} \right] \tag{KL divergence formula} \\
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - \log P(z|x) \right] \tag{From log identities} \\
&= \dots
\end{align*}

Under the hood, the KL divergence is just the expected value (expectation taken using probability $Q(z|x)$) of the logarithmic
difference between probabilities $Q(z|x)$ and $P(z|x)$.
\begin{align*}
\dots &= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - \log \frac{P(x|z) P(z)}{P(x)} \right] \tag{From Bayes' rule} \\
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - (\log P(x|z) + \log P(z) - \log {P(x)}) \right] \tag{From log identities} \\
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - \log P(x|z) - \log P(z) + \log {P(x)} \right] \tag{Removing parenthesis} \\
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - \log P(x|z) - \log P(z) \right] + \log {P(x)} \tag{P(x) independent from z} \\
&= \mathbb{E}_{z} \left[ \log Q(z|x) - \log P(z) \right] - \mathbb{E}_{z} \left[ \log P(x|z) \right] + \log {P(x)} \tag{From expected value linearity} \\
&= \dots
\end{align*}

The first addend of the right hand side (RHS) is now a KL divergence. We then move the last RHS term to the LHS, and invert the sign:
\begin{align*}
D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] &= D_\text{KL} \left[ Q(z|x) \| P(z) \right] - \mathbb{E}_{z \sim Q(z|x)} \left[ \log P(x|z) \right] + \log {P(x)} \\
D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] - \log {P(x)} &= D_\text{KL} \left[ Q(z|x) \| P(z) \right] - \mathbb{E}_{z \sim Q(z|x)} \left[ \log P(x|z) \right] \\
\log {P(x)} -
\underbrace{ D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] }_{\substack{\text{KL divergence between proposal} \\ \text{and, real, posterior distribution}}} &=
\underbrace{ \mathbb{E}_{z \sim Q(z|x)} \left[ \log P(x|z) \right] - D_\text{KL} \left[ Q(z|x) \| P(z) \right] }_{=\text{ELBO}}
\end{align*}

The RHS is the loss function of the VAE (next steps explain why and how), and is called \textbf{ELBO} (Evidence Lower BOund).
Remember that our objective is to learn a proposal distribution that approximates well the posterior distribution.

Analyzing the terms:
\begin{itemize}
  \item $\log {P(x)}$ is fixed given the dataset. Learning $P(x)$ is our final goal, but we want to do that according to the latent variable model
  (impractical in its base formulation, we stick to the variational inference framework). So\dots we don't care about this term during learning.
  \item $D_\text{KL}$ is intrinsically non-negative since it is a distance measure.
  \begin{itemize}
    \item $D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] \geq 0$
    \item $D_\text{KL} \left[ Q(z|x) \| P(z) \right] \geq 0$
  \end{itemize}
\end{itemize}

Results in:
\begin{align*}
&\log {P(x)} - D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] = \text{ELBO} \\
&\log {P(x)} =
\underbrace{D_\text{KL} \left[ Q(z|x) \| P(z|x) \right]}_{\geq 0} + \text{ELBO} \\
& \Rightarrow \log {P(x)} \geq \text{ELBO} \tag{Since $D_\text{KL}$ adds something positive to the ELBO}
\end{align*}

From the last implication, we can understand why the ELBO is a lower bound.
Therefore, by maximizing the ELBO, we are minimizing the $D_\text{KL}$ term, given that the true log-likelihood $\log {P(x)}$ is fixed.
The name \textit{variational} autoencoders comes from this: we are minimizing the variation between the two distributions.

\defbox{
  A \textbf{variational autoencoder} (VAE) is an autoencoder trained to maximize the \textbf{ELBO} (which becomes its loss function).
  Differently from the standard AE, the \textbf{encoder} $Q_\phi(z|x)$ of a VAE does not output a deterministic
  vector, but the params of the true posterior distribution $P(z|x)$.
  Complementary, the \textbf{decoder} $P_\theta(x|z)$ reconstructs the input $x$ from the sampled variable $z$.
}

% TODO diagram

% x -> Encoder -> q_{\phi}(z|x) -> Decoder -> \hat x

The loss function is the hence:
$$
\underbrace{\mathbb{E}_{z \sim Q(z|x)} \left[ \log P_\theta(x|z) \right]}_\text{Reconstruction term} -
\underbrace{D_\text{KL} \left[ Q_\phi(z|x) \| P(z) \right]}_\text{Regularization term}
$$

$P(z)$, which is the prior distribution, can be chosen by the network architect;
it is usually the normal distribution with zero mean and with a parametric variance:
$P(z) \rightarrow \mathcal{N} (z \ | \ \mu = 0, \sigma = I)$.
Therefore, the posterior distribution is alike a normal distribution with mean and variance learned during training.

In practical terms, the encoder network (which approximates $Q(z|x)$),
returns two vectors, representing the mean and the variance of a pseudo-normal distribution, given the input $x$:
$$ x \rightarrow \text{enc}(x; \phi) = [\mu(x), \sigma^2(x)] \quad \text{s.t.} \quad \mathcal{N} (z \ | \ \mu(x), \sigma(x)^2)$$

The decoder network (which approximates $P(x|z)$) is fed with $z$ and returns $\tilde{x}$:
$$ \text{dec}(z; \theta) \rightarrow \tilde{x} \quad \text{where} \quad z \sim \mathcal{N} (z \ | \ \mu(x), \sigma(x)^2) $$

To maximize $\log P(x|z)$ we can minimize the MSE between $x$ and $\tilde{x}$.

Note that the regularization term acts in the middle of the encoder, while the reconstruction term on the output.
We enforce a latent space to follow a standard Gaussian distribution.
If both posterior and prior distributions have Gaussian form, like in this case, the KL divergence as a closed-form equation.

How to generate new examples (a.k.a. perform inference)? \\
Option one, sampling from the posterior distribution $Q(z|x)$.\\
However, if training went well, the posterior $Q(z|X)$ is very similar to the prior $P(z)$. We can directly sample from the prior.

The process needed to generate new data (e.g., new images) is getting a sample from the prior
(a standard normal distribution), feeding it to the decoder, and getting its output.

Problem: \textbf{sampling} from a distribution is \textbf{not a differentiable operation},
so the previous pipeline is not end-to-end differentiable.

We use the re-parametrization trick to make the process backpropagation-safe:
$$
z \sim \mathcal{N} (z \ | \ \mu(x), \sigma(x)^2) \quad \text{becomes} \quad \epsilon \sim \mathcal{N}(0, I), \ Z = \mu + \sigma^2 \epsilon
$$

With this formulation, we still sample from a normal distribution, but the process is out of the gradient's path, making the
operation differentiable. $\epsilon$ is now just a sort of noise external to the gradient flow, allowing backpropagation.

\subsection{GANs}
GANs stands for \textbf{Generative Adversarial Networks}.
Note the final s: multiple networks will be there.

Starting point: the images generated by VAEs are often blurry.

Differently from VAEs, in GANs, we have no explicit density function.
We learn to map noise $z \sim P(z)$ directly to real data via \textbf{adversarial training}.

We train the network to transform a noise vector $z$ sampled from a distribution $P(x)$ to some data that look alike the original samples,
through a learned transformation $G(z)$.

Two actors:
\begin{itemize}
  \item \textbf{Generator}: generates data as close as possible to reality.
  \item \textbf{Discriminator}: given a data from the generator, has the objective to determine if it comes from
  the generator or the real dataset, a.k.a. if it is real or fake.
\end{itemize}

During the training, the generator will learn how to fool the discriminator.
The generator improves over training, it learns how to generate fake data more and more similar to real ones.
At the same time, the discriminator improves: it gets better and better in distinguishing if a data is real or fake.
The two networks are therefore \textbf{competing} against each other.

This is a minimax problem, since the two players go towards different objectives:
$$
\min_{\theta_g} \max_{\theta_d} \ [
\underbrace{ \mathbb{E}_{x \sim P_\text{data}(x)} \log D_{\theta_d} (x) }_{\substack{\text{Likelihood of the discriminator}\\\text{predicting real data as real}}} + 
\underbrace{ \mathbb{E}_{z \sim P(z)} \log(1 - D_{\theta_d}(G_{\theta_g}(z))) }_{\substack{\text{Likelihood of the discriminator}\\\text{predicting generated data as fake}}}
]
$$

Ideally, we would like to have a discriminator $D$ that always flags real data as genuine (1 = real), and generated data as fake (0 = fake).
So\dots by \textbf{maximizing} the function above we are asking $D(x)$ to be as close to 1 as possible and $D(G(z))$ to be as close
to zero as possible:
$$
\max_{\theta_d} \ [ \mathbb{E}_{x \sim P_\text{data}(x)} \log D_{\theta_d} (x) + \mathbb{E}_{z \sim P(z)} \log(1 - D_{\theta_d}(G_{\theta_g}(z))) ]
$$

With gradient descent we do not know how to maximize a quantity.
We can put a minus in front of the loss and use gradient descent, or use gradient ascent.

The generator has the \textbf{opposite interest}: minimizing the likelihood of the discriminator discovering its fake images.
We fix the set of parameters (just $\theta_g$ under the $\min$) of the discriminator, and we optimize (the only portion
of the objective function that depends on $G$):
$$
\min_{\theta_g} \ [ \mathbb{E}_{z \sim P(z)} \log(1 - D_{\theta_d}(G_{\theta_g}(z))) ]
$$

If the discriminator is too good at discriminating ($D_{\theta_d}(G_{\theta_g}(z))$ close to 0, which means
$1 - D_{\theta_d}(G_{\theta_g}(z))$ close to 1) and the generator is
not generating realistic images yet, the gradient received by the generator becomes almost zero (indeed $\log(1 - D_{\theta_d}(G_{\theta_g}(z)))$
is close to 0), and it will not improve its quality, leading to \textbf{vanishing gradient}.

As a solution, we can maximize $\log(D_{\theta_d}(G_{\theta_g}(z)))$ instead of minimizing $\log(1 - D_{\theta_d}(G_{\theta_g}(z)))$;
the optimization problem is the same, but the \textbf{gradient signal is higher} during initial training phases,
in which the generator is still not good at generating realistic images.

\begin{algorithm}
  \caption{Minibatch SGD of a GANs}
  \begin{algorithmic}[1]
    \For{training iterations}
      \For{$k$ steps}
        \State Sample minibatch of $m$ noise samples from noise prior $P_g(z)$: $z \sim P_g(z) \in \mathbb{R}^{(m)}$
        \State Sample minibatch of $m$ examples from dataset $P_\text{data}(x)$: $x \sim P_\text{data}(x) \in \mathbb{R}^{(m)}$
        \State Update \textbf{discriminator} params by ascending (maximizing) stochastic gradient:
        $$
        \nabla_{\theta_d} \ \frac{1}{m} \sum_{i=1}^m \left[ \log D_{\theta_d} (x^{(i)}) + \log(1 - D_{\theta_d}(G_{\theta_g}(z^{(i)}))) \right]
        $$
      \EndFor

      \State Sample minibatch of $m$ noise samples from noise prior $P_g(z)$: $z \sim P_g(z) \in \mathbb{R}^{(m)}$
      \State Update \textbf{generator} params by ascending (maximizing) stochastic gradient:
      $$
      \nabla_{\theta_g} \ \frac{1}{m} \sum_{i=1}^m \log(D_{\theta_d}(G_{\theta_g}(z^{(i)})))
      $$

    \EndFor
  \end{algorithmic}
\end{algorithm}

The number of steps $k$ of generator and discriminator is a hyperparameter and can be different.
The exact number of steps depend on how the generator compares with respect to the discriminator.

Be aware that in GANs we don't have explicit distribution parameters, contrary to the VAEs.
We cannot get the explicit distribution, just sample from it implicitly.

By performing arithmetic operation in the latent space we can control the semantic of the generated output.\\
E.g., mixing the latent vector corresponding to the input that gives \textit{men with glasses}, subtracting the
vector that gives \textit{men}, and adding the input of \textit{women}; we should obtain women with glasses.

\subsection{Evolution of GANs}
A series of further improvements regarding the training stability has been published in literature.

CycleGAN performs image-to-image translation, specifically, it translates an input image to the equivalent in a target style (e.g., Monet painting).
Up to this paper, this kind of networks were trained with paired examples, an image on the input domain $X$ and an image on the output domain $Y$.
This work presents an approach for learning translation without paired examples.
``Cyclic'' comes from the fact that an additional constraint is added (otherwise, the problem would have been underconstrained):
the inverse mapping $Y \rightarrow X$ has to be learned by the network; the loss is in fact called \textit{cyclic consistency} loss.
Two different adversarial discriminators are then used: $D_Y$ and $D_X$.
The cyclic nature of the network is an attempt of regularization.

Problem: GANs provide a fixed resolution output.\\
Solution: progressive GANs trains at low resolution and progressively increase resolution up to full scale.
At each step we increase resoluiton, e.g., by doubling the size, and retrain the network with an additional layer.
This way, the network will become capable of generating images at different resolution, even at high ones, without wasting too much 
memory and computing power. The previously trained layers will give a good starting point to the next training step.
NVIDIA was able to generate 1 Megapixels images.

StyleGANs are style-guided generators. They exploit a style vector plugged into the image with AdaIN.
The more the styles are disentangled, the more they are composable.
Disentangled means dot product near zero. If they are not disentangled one style will overwrite the other one.