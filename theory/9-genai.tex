\section{Generative Models}
\subsection{Background}
In this section, the concept of a \textbf{marginal distribution} is often referenced.

\defbox{
  A \textbf{marginal distribution} is the distribution we obtain aggregating (by summing or integrating)
  the original probability distribution over a subset of the original variables; the resulting
  distribution will no longer be parametrized over the aggregated variables (``ignoring'' them).
}

Intuitively, it consists in flattening a high-dimensional space into a lower dimensional one,
reducing the degrees of freedom of the probability function.

The resulting distribution is called ``marginal'' because in the days of paper tables, mathematicians
used to sum up rows or columns on the margins of a page.

E.g.:\ given the joint probability function $P(S = \text{suit}, N = \text{number}) = \frac{1}{52}$
of drawing a card with the specified suit and number from a deck of 52 poker cards, if we
marginalize out $N$ we get
$P(S = \text{suit}) = \sum_{n \in {2, 3, \dots, K, A}} P(S = \text{suit}, N = n) = \frac{1}{4}$.

\subsection{Objective}
This section is about \textbf{variational autoencoders} (VAEs) and \textbf{generative adversarial networks} (GANs).

Given a dataset, we want to learn the \textbf{probability distribution} $P(x)$ of the datapoints in
the dataset, and \textbf{sample} from that distribution to generate new datapoints.\\
E.g., form a dataset of cat images, learn to generate (\textbf{synthesize}) the image of a new cat.

Two different applications are:
\begin{itemize}
  \item \textbf{Sampling}: sample new example $x ~ P(x)$ from the probability distribution.
  \item \textbf{Inference}: process of computing the probability $P(x)$ of a datapoint $x$ belonging to the distribution.
  This enables anomaly detection.
\end{itemize}

Denoising and contractive autoencoders can implicitly learn the probability distribution of the dataset,

\subsection{VAE}
Stands for \textbf{Variational Autoencoder} (VAE).

Denoising and contractive autoencoders can implicitly learn the probability distribution $P(x)$,
however, it is very difficult to sample new examples from these models.

Given the task of cat images generation,
the simplest generative model is a model that learns $\mathcal{N}(X \ | \ \mu_\text{cat}, \Sigma_\text{cat})$
for each pixel, and samples from it.
This is an overly simple distribution, plus we are assuming pixels independence.
This generative model operates over the pixel space (the same space of the observation in which the observation lives),
which is not optimal: we could keep the simple distribution
assumption and change just the space on which we operate.

\defbox{
  \textbf{Latent variable models} are models in which we relate a set of observable variables to a set of latent variables, so that
  the that we can derive an observable data $x$ from $z$: $z \rightarrow x$.
  Latent variables represent hidden quantities, that can explain the observable data.
}

% TODO: diagram phi, z, x, phi.

Since we cannot observe $z$, the observed data distribution is obtained by marginalizing over $z$:
$$
\underbrace{P(x)}_\text{marginal probability} = \quad
\int \underbrace{P(x, z)}_\text{joint probability} \ \text{d} z \quad
\underbrace{= \int P(x | z) P(z) \ \text{d} z}_\text{from Bayes' theorem}
$$
Explanation: integral of all $z$s conditioned on joint probability $P(x, z) = P(x \cap z)$.

This is an \textbf{intractable} problem, since it is an indefinite integral (upper and lower unbounded), on a continous space.
We move to a discrete space and apply \textbf{Monte Carlo integration} to solve it:
$$
P(x) \approx \frac{1}{K} \sum_{k=1}^K P(x | z^{(k)}), \quad z^{(k)} \sim P(z)
$$

The \textbf{larger} the number of samples $K$, the better the approximation.
Notice that the explicit weighting of the previous formula becomes an implicit weighting; the distribution of the drawings
from $P(z)$ influences the final result through $P(x | z^{(k)})$.
We obviously have to learn $P(x | z)$ and $P(z)$ from the dataset.

Sampling procedure:
\begin{enumerate}
  \item sample $z$ from $P(z)$
  \item sample $x$ from $P(x|z)$
  \item integrate with Monte Carlo
\end{enumerate}

Sampling from the prior distribution $P(z)$ is inefficient: the portion of the distribution from which we sample,
has a huge impact on the final result.
We want to sample from points that are significant for the observation $x$; however, most of the sampled $z$s are
irrelevant to $x$. To get a good estimate of $P(x)$ we would need a large number of samples (inefficiency).

Idea: learn a model that, given an observation $x$, predicts the most plausible latent variables $z$. Meaning,
those latent variables that are more related to $x$ under the posterior distribution $P(z|x)$.

Solution: \textbf{variational inference}\\
Idea: modeling the ture posterior distribution $P(z|x)$ with a simpler distribution, the \textbf{proposal distribution} $Q(z|x)$.

The \textbf{proposal distribution} is a simpler, easier to evaluate distribution, from which we sample instead of $P(z)$.
This distribution provides good sampling points, that allows to estimate $z$ with fewer samples.
Instead of sampling $z$ from $P(z)$ (step 1), we sample from $Q(z|x)$.

Clearly, we want $Q(z|x)$ to be as close as possible to the true posterior $P(z|x)$; in other words, we want
their \textbf{divergence}** to be \textbf{as low as possible}:
$$
Q*(z|x) = \arg \min_Q D_\text{KL} \left[ Q(z|x) \| P(z|x) \right]
$$

** $D_\text{KL}$ is Kullback-Leibler divergence. The analogous of euclidean distance but for distributions:
\begin{align*}
D_\text{KL} \left[ Q(z|x) \| P(z|x) \right]
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log \frac{Q(z|x)}{P(z|x)} \right] \tag{KL divergence formula} \\
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - \log P(z|x) \right] \tag{From log identities} \\
&= \dots
\end{align*}

Under the hood, the KL divergence is just the expected value (expectation taken using probability $Q(z|x)$) of the logarithmic
difference between probabilities $Q(z|x)$ and $P(z|x)$.
\begin{align*}
\dots &= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - \log \frac{P(x|z) P(z)}{P(x)} \right] \tag{From Bayes' rule} \\
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - (\log P(x|z) + \log P(z) - \log {P(x)}) \right] \tag{From log identities} \\
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - \log P(x|z) - \log P(z) + \log {P(x)} \right] \tag{Removing parenthesis} \\
&= \mathbb{E}_{z \sim Q(z|x)} \left[ \log Q(z|x) - \log P(x|z) - \log P(z) \right] + \log {P(x)} \tag{P(x) independent from z} \\
&= \mathbb{E}_{z} \left[ \log Q(z|x) - \log P(z) \right] - \mathbb{E}_{z} \left[ \log P(x|z) \right] + \log {P(x)} \tag{From expected value linearity} \\
&= \dots
\end{align*}

The first addend of the right hand side (RHS) is now a KL divergence. We then move the last RHS term to the LHS, and invert the sign:
\begin{align*}
D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] &= D_\text{KL} \left[ Q(z|x) \| P(z) \right] - \mathbb{E}_{z \sim Q(z|x)} \left[ \log P(x|z) \right] + \log {P(x)} \\
D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] - \log {P(x)} &= D_\text{KL} \left[ Q(z|x) \| P(z) \right] - \mathbb{E}_{z \sim Q(z|x)} \left[ \log P(x|z) \right] \\
\log {P(x)} -
\underbrace{ D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] }_{\substack{\text{KL divergence between proposal} \\ \text{and, real, posterior distribution}}} &=
\underbrace{ \mathbb{E}_{z \sim Q(z|x)} \left[ \log P(x|z) \right] - D_\text{KL} \left[ Q(z|x) \| P(z) \right] }_{=\text{ELBO}}
\end{align*}

The RHS is the loss function of the VAE (next steps explain why and how), and is called \textbf{ELBO} (Evidence Lower BOund).
Remember that our objective is to learn a proposal distribution that approximates well the posterior distribution.

Analyzing the terms:
\begin{itemize}
  \item $\log {P(x)}$ is fixed given the dataset. Learning $P(x)$ is our final goal, but we want to do that according to the latent variable model
  (impractical in its base formulation, we stick to the variational inference framework). So\dots we don't care about this term during learning.
  \item $D_\text{KL}$ is intrinsically non-negative since it is a distance measure.
  \begin{itemize}
    \item $D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] \geq 0$
    \item $D_\text{KL} \left[ Q(z|x) \| P(z) \right] \geq 0$
  \end{itemize}
\end{itemize}

Results in:
\begin{align*}
&\log {P(x)} - D_\text{KL} \left[ Q(z|x) \| P(z|x) \right] = \text{ELBO} \\
&\log {P(x)} =
\underbrace{D_\text{KL} \left[ Q(z|x) \| P(z|x) \right]}_{\geq 0} + \text{ELBO} \\
& \Rightarrow \log {P(x)} \geq \text{ELBO} \tag{Since $D_\text{KL}$ adds something positive to the ELBO}
\end{align*}

From the last implication, we can understand why the ELBO is a lower bound.
Therefore, by maximizing the ELBO, we are minimizing the $D_\text{KL}$ term, given that the true log-likelihood $\log {P(x)}$ is fixed.
The name \textit{variational} autoencoders comes from this: we are minimizing the variation between the two distributions.

\defbox{
  A \textbf{variational atuoencoeder} (VAE) is an autoencoder trained to maximize the \textbf{ELBO} (which becomes its loss function).
  Differently from the standard AE, the \textbf{encoder} $Q_\phi(z|x)$ of a VAE does not output a deterministic
  vector, but the params of the true posterior distribution $P(z|x)$.
  Complementary, the \textbf{decoder} $P_\theta(x|z)$ reconstructs the input $x$ from the sampled variable $z$.
}

% TODO diagram

% x -> Encoder -> q_{\phi}(z|x) -> Decoder -> \hat x

The loss function is the hence:
$$
\underbrace{\mathbb{E}_{z \sim Q(z|x)} \left[ \log P_\theta(x|z) \right]}_\text{Regularization term} -
\underbrace{D_\text{KL} \left[ Q_\phi(z|x) \| P(z) \right]}_\text{Reconstruction term}
$$

$P(z)$, which is the prior distribution, can be chosen by the network architect;
it is usually the normal distribution with zero mean and with a parametric variance:
$P(z) \rightarrow \mathcal{N} (z \ | \ \mu = 0, \sigma = I)$.
Therefore, the posterior distribution is alike a normal distribution with mean and variance learned during training.

In practical terms, the encoder network (which approximates $Q(z|x)$),
returns two vectors, representing the mean and the variance of a pseudo-normal distribution, given the input $x$:
$$ x \rightarrow \text{enc}(x; \phi) = [\mu(x), \sigma^2(x)] \quad \text{s.t.} \quad \mathcal{N} (z \ | \ \mu(x), \sigma(x)^2)$$

The decoder network (which approximates $P(x|z)$) is fed with $z$ and returns $\tilde{x}$:
$$ \text{dec}(z; \theta) \rightarrow \tilde{x} \quad \text{where} \quad z \sim \mathcal{N} (z \ | \ \mu(x), \sigma(x)^2) $$

To maximize $\log P(x|z)$ we can minimize the MSE between $x$ and $\tilde{x}$.

Note that the regularization term acts in the middle of the encoder, while the reconstruction term on the output.
We enforce a latent space to follow a standard Gaussian distribution.
If both posterior and prior distributions have Gaussian form, like in this case, the KL divergence as a closed-form equation.

How to generate new examples (a.k.a. perform inference)? \\
Option one, sampling from the posterior distribution $Q(z|x)$.\\
However, if training went well, the posterior $Q(z|X)$ is very similar to the prior $P(z)$. We can directly sample from the prior.

The process needed to generate new data (e.g., new images) is getting a sample from the prior
(a standard normal distribution), feeding it to the decoder, and getting its output.

Problem: \textbf{sampling} from a distribution is \textbf{not a differentiable operation},
so the previous pipeline is not end-to-end differentiable.

We use the re-parametrization trick to make the process backpropagation-safe:
$$
z \sim \mathcal{N} (z \ | \ \mu(x), \sigma(x)^2) \quad \text{becomes} \quad \epsilon \sim \mathcal{N}(0, I), \ Z = \mu + \sigma^2 \epsilon
$$

With this formulation, we still sample from a normal distribution, but the process is out of the gradient's path, making the
operation differentiable. $\epsilon$ is now just a sort of noise external to the gradient flow, allowing backpropagation.

% \subsection{GANs}
% Generative Adversarial Netowrks
% Note the final s: multiple networks will be there.

% Problem: the images generated by VAEs are 

% Differently from VAEs, we have no explicid density function. We learn to map noize z ~ p(z) directly to real data via adversarial training.

% We train the network to transform a noise vector z ~ p(z) sampled from a distribution to some data that look alike the original ones.

% Two actors:
%  - generator: generates data as close as possible to reality
%  - discriminator: given a data from the generator, has the objective to determine if it comes from generator or real dataset, aka if it is real or fake

% During training, generator will learn how to fool the discriminator.
% The generator improves over training, it learns how to generate fake data really similar to real ones.
% At the same time, the discriminator improves. It gets better and better in distinguishing if a data is real or fake.

% This is a minmax problem, since the two players go towards different objectives:
% minmax[ accuracy/likelihood of distriminator on real data + accuracy/likelihood of distriminator on generated data] TODO complete formula
% The second one is the only one where G is present.

% With gradient descent we do not know how to maximaze a quantity. We can put a minus in front of the loss or use gradient ascent,
% which is the same as descent, whith the only difference that there is a plus instead of a minus in front of the gradient.

% Notice that one is derived wrt theta_G and the other one wrt theta_D.
% In PyTorch terms corresponds to different calls to backward().

% If the discriminator is too good at discriminating and the generator is not generating realistic images yet,
% the gradient received by the generator becomes zero and do not improves quality too early.
% We can maximize log(D(G(z))) instead of minimizing 1-log(D(G(z))); the optimization problem is the same, but the gradient signal is higher
% during initial training phases, in which the generator is still not good at generating realistic images.

% TODO trainig algorithm code

% The number of training steps of generator and discriminator can be different.
% The exact number of steps depend on how the generator compares with respect to the discriminator.

% Be aware that in GANs we don't have explicit distribution parameters, contrary to the VAEs.
% We cannot learn the distribution, just learn from it.

% By combining the inputs of the GANs we can obtain combined images from different classes.
% Eg. mixing the input that gives men with glasses, sustracting the input that give men, and adding the input of women; we should obtain
% women with glasses.

% Evolution of GANs.
% A series of further improvements regarding the training stability has been published in literature.

% CycleGAN performs image-to-image translation, specifically, it translate an input image to the equivalent in a target style (eg. Monet painting).
% Up to this paper this kind of networks were trained with paired examples, an image on the input domain X and an image on the output domain Y.
% This work presents an approach for learning translation without paired examples.
% "Cyclic" comes from the fact that an additional constraint is added (otherwise, the problem would have been constrained): the inverse mapping
% Y -> X has to be learned by the network; the loss is in fact called \textit{cyclic consistency} loss.
% Two different adversarial discriminators are then used: D_Y and D_X.
% The cyclic nature of the network is a way of regularazing.

% Problem: GANs provide a fixed resolution output.
% Solution: progressive GANs trains at low resolution and progressively increase resolution up to full scale.
% At each step we increase resoluiton, eg. by doubling size, and retrain the network with an additional layer.
% This way, the network will become capable of generating images at different resolution, even at high ones, without wasting too much 
% memory and computing power. The previously trained layers will give a good starting point to the next training step.
% Nvidia was able to generate 1Mpixels images.

% StyleGANs are style-based genertors. It exploits a style vector plugged into the image with AdaIN.
% AdaIN is basically batch normalization plus a rescale and shift whose parameters Y are given by an MLP:
% adain(Y) = Y_s \frac{x - \mu_B}{\sigma_B} + Y_B.

% The more the styles is disentangled, the more they are composable.
% Disentangled means dot product near zero. If they are not disentangled one style will overwrite the other one.